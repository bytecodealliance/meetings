# Wasm+ML Working Group &mdash; February 5

See the [instructions](../README.md) for details on how to attend.

### Agenda

1. Opening, welcome and roll call
    1. Please help take notes &mdash; thanks!
1. Announcements
    1. _Submit a PR to add your announcement here_
1. Proposals and discussions
    1. describe use case for streaming tensor results in wasi-nn; i.e., `compute_single` solution;
       `hydai` (30 minutes, [slides][hd-slides])
    1. describe requirements for model versioning; what is needed in the embedded space? Ayako
       Akasaka (15 minutes, [slides][aa-slides])
    1. describe improvements for tensor data validation--extract applicable part of solution from
       `bytes` discussion; Matthew-Tamayo Rios (10 minutes, [issue][mtr-issue])

[hd-slides]: ML-02-05-hd-slides.pdf
[aa-slides]: ML-02-05-aa-slides.pdf
[mtr-issue]: https://github.com/WebAssembly/wasi-nn/issues/62

### Attendees

- Brown, Andrew
- Hung-Ying Tai (hydai)
- Sun, Mingqiu
- Stuart Schaefer
- Tangri, Saurabh
- Ayako Akasaka
- Matthew Tamayo-Rios (MTR)
- Wilson Wang

### Notes

Below is a transcript of the discussion as we progressed through the agenda items. Some high-level
points:
- hydai pointed out a need for key-value metadata at every wasi-nn level (loading, context, compute)
  to support LLMs
- hydai noted the need for destroying contexts; this may be possible with "contexts as WIT
  resources"
- hydai pointed out the need to stream results; until WIT accepts generics, wasi-io [will not
  allow][streams] returning streams of opaque tensors
- Ayako described the need to discover device capabilities (even beyond wasi-nn); there is a
  question of in-band (i.e., inside a WebAssembly module) vs. out-of-band
- Matthew discussed the need for a better experience when using non-Wasm tensor types (e.g., BF16,
  FP16, etc.); we agreed to continue the discussion on MTR's [issue][mtr-issue] in the next meeting.

[streams]: https://github.com/WebAssembly/wasi-io/blob/main/wit/streams.wit#L59-L62

-----

hydai: Can you guys see my sharing screen?

Brown, Andrew: Yep.

hydai: OK, so today I'm going to show how you know how we ohh half a walk around to do the a stream result when execution LM model.

hydai: So the original question is that if you want to execute a LM inference on a limited device, for example your, you know your PC GPU, you will test times.

hydai: OK, so for some application, especially the API, it's over.

hydai: Uh, it's pretty hard to wait.

hydai: The whole result finish because if you we figure out that the result is not correct or is not really related to what we expect, we want to, you know terminate that yet and then go through to the next request, right.

hydai: So we have some work around to fix this issue.

hydai: So in this agenda, I will talk about some.

hydai: No, we we just back an extension from the original what's the NN spec and the part will, you know spread in three component.

hydai: The first one is that we.

hydai: We package the configuration for the LLM into a metadata and we add some.

hydai: You know a function to compute the single token and get a single token then because that's in the original was an spec.

hydai: There is no way to unload a model.

hydai: Umm, you know when we start an API server, the different LM model can handle the different.

hydai: Ohh issue or different question.

hydai: For example, if we want to.

hydai: Ohh.

hydai: Identifies or do the OCR job front photo.

hydai: We may need lava model, however, if we want to do the chat model, we may use the two chat model.

hydai: So we have to unload the the contest, unload the model and then reload a new one.

hydai: So we have the a final state.

hydai: Uh.

hydai: Function to do that and we also add some you know customers errors because uh, I I think in the current spec we have no way to define and customer error.

hydai: So yeah, let's all we modified from the original spec.

hydai: So let's talk about the first thing is the metadata.

hydai: It's pretty similar.

hydai: You know, we just because of the input function is pretty good.

hydai: Yeah, we can say lots of, you know, the lots of input with the different index.

hydai: So we always tackle the identity role as the, you know, the rolling build for the model and we tag the index one to store the just average, which is if you want to enable the embedded or you know lots of different options for the LLM.

hydai: You can just sell just object and then umm it was the original set input.

hydai: There's pretty good, right?

hydai: And also because when the model produced the output actually also produce some, you know, different scenes.

hydai: For example, you will produce a.

hydai: How many input token you use?

hydai: How many output token you use and lots of different metadata?

hydai: So we also do the same thing.

hydai: Uh, in the, you know, get output function which is we always use the index 0 as a rule output from the model and also we have the index one to store the the return metadata.

hydai: Yeah.

hydai: And I believe we because when you want to, you know, load a LLM, you have to set up lots of different.

hydai: Uh.

hydai: Options when you load it.

hydai: Let's say if you want to sell the temperature, nurture, or you want to sell the contact size.

hydai: Lots of thing.

hydai: So we also extend a function code you know just low model with configuration.

hydai: The configuration is defined, you know in this metadata just format.

hydai: OK, so the the only thing we do is that we add a new encoding called GGML for those guys you want to, you know, knowledge, UFD model and we add a configuration to make sure the model is loaded only once with the right configuration you give and then when we go to the you know initialize the contest.

hydai: Actually, this situation is happened when you run an API server because if we want to do the you know something local copilots staff you may have the different content sites.

hydai: For example, if you are call Stamper is pretty small, you can sell a little content size and you can set a little M predict to reduce the a computer resource and get the results faster, right?

hydai: So we allow you to, you know, to modify the metadata in each computation before the each computation.

hydai: You can override it and we will destroy the context and reconstruct our new contacts for you.

hydai: Yeah.

hydai: So that's some something we also add and I believe this is the most important part because that, uh, in the main group we have to, we don't want to block here in the original.

hydai: Ohh, in the original design of the was an end.

hydai: We will call the computer function and it will just hand until the result is finished.

hydai: However, you know in most of the LLM staff especially you are using for example the chat GPT, you can always stop it in any time.

hydai: You just don't want.

hydai: Uh, you.

hydai: It goes through to produce a new output, so so we add a protein function.

hydai: You can call the computer single.

hydai: You will always computer the next token and you can get the next token directory from the output buffer.

hydai: So when we go to the, you know the the porting mode.

hydai: Because we still cannot have an asynchronized solution in the Watson side.

hydai: So actually in the current situation, we combine the pruning function with a you know a a timer.

hydai: So we can, for example, we can check every second if the request want to be cancelled and then we will stop this loop and go through yeah.

hydai: And we also add, you know several new errors because those errors are only happened when you use the alarm.

hydai: If you are using some, you know some traditional inference model you you don't have encountered.

hydai: For example, the the contest will never be full, right?

hydai: Yeah.

hydai: So, so we we just add this thing and the most important part is that because if you if you are not deleting the previous contest, it will keep the state.

hydai: So, uh, if you are running the API server, you ask some question and you will memory you will you will recall the previous response and another request comes.

hydai: Then you will, you know, produce some new response depends on the previous one.

hydai: So we have to, you know, add a new AA finalize function to avoid this situation happen.

hydai: Yeah.

hydai: So let's all we did as a workaround as an extension from the original Wasi NN, when we handle it API server, especially in the streaming mode.

hydai: So if you are interested in, I attach the you know the compare function.

hydai: Sorry the compare patch so you can find that how many extra function or extra type we add in the following link.

hydai: Yeah.

hydai: So that's all I have and if you are, you have any question, just let me know.

hydai: Thank you.

Brown, Andrew: Thank you so much.

Brown, Andrew: Heidi does.

Brown, Andrew: Does anyone have any questions?

Brown, Andrew: I I can I can kick it off.

Brown, Andrew: I I sort of tected this.

Brown, Andrew: It seemed to me like you needed metadata.

Brown, Andrew: OK.

Brown, Andrew: Right now, wasna kind of has three levels, right?

Brown, Andrew: You load a model, then you create a context and then you like compute inside that context.

Brown, Andrew: There's kind of three levels and it seemed to me you needed metadata at each level.

Brown, Andrew: You needed a config when you load it.

Brown, Andrew: I think you needed some way to add configuration to the context and you also mentioned that in set inputs get output you you need you you you were setting the the the next the final index as like a a metadata thing.

Brown, Andrew: Is that correct?

hydai: Yes, because when it is a very interesting thing, when you load the model, you have to sell some option and then if you create the context because the contest will define the context, size, how many contacts you want, because the have is training contacts.

Brown, Andrew: Mm-hmm.

hydai: For example, the the.

hydai: Yeah, just just the value and because if you are running the API server.

hydai: Ohh very important thing is that you have to set the reverse prompt to know the model is you know reached to the the the end result or you want to know the how many token is generated.

Brown, Andrew: Yeah.

hydai: Because if you want to run an open AI compared server, you have to do that less is defined in is you know it's API response.

hydai: Yeah.

hydai: So we just, you know, because we we we seem to create 3 metadata objects to heavy and it is unnecessary.

hydai: So we just category all of the staff into one just object, yeah.

Brown, Andrew: OK.

Brown, Andrew: And you know those that metadata at each level, it's key value stuff, right?

hydai: Yeah.

Brown, Andrew: The values are.

hydai: Ohh.

Brown, Andrew: Uh, what kinds of values as it is it mainly just integers or is it like anything?

hydai: It could be anything.

hydai: Let's say if you are doing the contact size then it is you know it is integer right?

Brown, Andrew: Yeah.

Brown, Andrew: OK.

hydai: But if you are putting the embedding, the embedding vector is a floating point number array, and if you are putting some you know reverse prompts.

hydai: That's a stream.

hydai: Yeah.

hydai: So we just encoding everything into just object and serializing it and then we do the decoding job in our.

hydai: You know, in our was in the back end, yeah.

Brown, Andrew: OK, cool.

Brown, Andrew: Makes sense.

Brown, Andrew: It seems kind of like you know your.

Brown, Andrew: There's a there's a.

Brown, Andrew: There's a one to one between the the the chat session and the wasin and context, right?

hydai: Yeah, yeah.

Brown, Andrew: And and you and that's why you need to drop the wheezing and context when that session is done right?

Brown, Andrew: Because you didn't want results from this interfering with results later, umm.

hydai: Uh, actually, because that if you are doing a conversation, you may put some, you know, sensitive data into it and you you don't want to link it to another people, right? Yeah.

Brown, Andrew: Yeah.

Brown, Andrew: Do you want you want the?

Brown, Andrew: You want that to be forgotten like that.

hydai: Yeah.

hydai: Yeah, yeah, yeah, yeah.

Brown, Andrew: That context is dropped that sessions over.

Brown, Andrew: I wonder if if the if the context are resources.

Brown, Andrew: Resources can be dropped.

Brown, Andrew: They can be finalized, they can be like destroyed.

Brown, Andrew: And so I wonder if that will kind of resolve on its own as we like sort of move towards resources once the contexts are researched, you could in rust, you should just be able to drop the, the, the context and it should be finalized.

Brown, Andrew: So maybe maybe for that one, we don't need the new the the new API for now.

Brown, Andrew: Yeah.

hydai: Uh, yeah, I believe you've if we refine the, you know, the context as a resource type and maybe the life cycle is and you know is is naive to do that and yeah, but but this is in the you know in the LLM situation.

hydai: I'm not sure if you are running other model.

Brown, Andrew: Yeah.

hydai: Will this do?

hydai: We really need to job the contest.

hydai: I don't know. Yeah.

Brown, Andrew: Yeah, yeah.

Brown, Andrew: Does anyone else have any other questions?

Brown, Andrew: Because I was, I was gonna dig into the streaming part of this.

Sun, Mingqiu: Yeah.

Sun, Mingqiu: So one question I have you know, so I think what you describe makes sense for large language model, but just wondering whether the API will make sense for you know other a machine learning use cases.

Sun, Mingqiu: You know, for example for image classification.

Sun, Mingqiu: So compute single and a compute will be and then they go or you do like a multiple step for image classification.

Sun, Mingqiu: Have you have you guys thought about like A is this applicable for all the AI problems?

hydai: Umm, I don't think so because that's if we are running the image classification the the output will not be a single token.

hydai: Follow a single token right?

hydai: Because there there's not how the model, those kinds of model works.

hydai: So let's why we treat this one as extension, because I I don't think this is uh, you know, a good way to merge back into the the Wasi NN spec because this is not a general solution right now.

hydai: And I also want to figure out, you know, uh, if we can have a general solution to apply some kind of list to all of the different kinds of the AI.

hydai: Uh, uh tasks.

hydai: Yeah.

hydai: So the answer I will say no, I don't think so.

Brown, Andrew: You know, Speaking of the general solution I I've mentioned this in the past sort of floated the idea without like looking into it too deeply, but I'd sort of thought ohh streams streams are the solution that Luke Wagner has been talking about to sort of.

Brown, Andrew: Uh, you know, if we want, if we want us to split up, you know, results over time, we could send tensors as, like different events in a stream.

Brown, Andrew: So today I I looked up.

Brown, Andrew: You know what?

Brown, Andrew: The API is for a stream and I'll paste it here in the chat window.

Brown, Andrew: Uh.

Brown, Andrew: The and if if you click that link what you'll see is like the read function of an input stream and you'll see that it it accepts a length as a parameter and it returns a list of U8 and.

Brown, Andrew: So.

Brown, Andrew: So what's what's good about, you know, these IO streams is that they're sort of the precursor to the real async streams that one day will be present in WIT.

Brown, Andrew: And so I thought, man, it would be great to get on inside this paradigm.

Brown, Andrew: So that one day, you know, it's an easy transition into like the full stream.

Brown, Andrew: The problem I see with this though is you know, though the concept is a general solution to what Heidi is describing, the specifics don't like fit super well, right?

Brown, Andrew: We don't actually wanna return lists of U8 here, or at least I didn't think we did.

Brown, Andrew: I thought we wanted to return tensors and we can't really do that with the WIT language right now.

Brown, Andrew: There's no way of having generic types on these functions so that Reed could return a stream of tensors instead of a stream of just arrays.

Brown, Andrew: Uh.

Brown, Andrew: And that makes me think.

Brown, Andrew: Ohh, maybe we can't jump on to the the stream bandwagon just yet.

Brown, Andrew: And if we do wanna create a general solution, we might need to design one that is compatible eventually with async streams when they are available, but that in the meantime kind of is is like separate from these IO streams.

Brown, Andrew: Go ahead, Stewart.

Stuart Schaefer: I I I just.

Stuart Schaefer: I just wanna clarify one thing.

Stuart Schaefer: I think the part of what you said that makes me a little nervous is that when we are two concrete about stream of tea and what tea has to be, that's what you create kind of a problem like I think I mentioned before what I'm looking for for GPU is I wanna pass it and opaque handle right.

Stuart Schaefer: That says this is a region of memory and a GPU, and I'm hoping you don't touch it right now.

Stuart Schaefer: You know, I'm hoping that it passes straight through web assembly without anyone trying to do anything with it because it's a bad idea, right?

Stuart Schaefer: And so I think one of the things that's hard here is that facing stream of or stream of tea where tea is not, you know, U8 or some specific wasm type.

Stuart Schaefer: You know, that's my question to you is like, do you see that or do you see that everything here that we build has to be well it's of tensor but tensor at some point grounds out to something has to be bytes has to be whatever as opposed to handle of right?

Brown, Andrew: Well, my my take was tensors are our way of making things opaque, right?

Brown, Andrew: Whereas list U8 is making things very concrete and you know observable.

Brown, Andrew: So with a tensor we would be able to eventually say, hey, look, you can't actually like touch these bytes right now at a certain point, like maybe you could, maybe you can observe them or not, right?

Brown, Andrew: Umm, but we couldn't do that with a list. U8.

Brown, Andrew: Uh.

Brown, Andrew: Maybe there's different types of tensors?

Brown, Andrew: Uh.

Brown, Andrew: For tensors, that can be like observed and poked on, and tensors that can't.

Brown, Andrew: So I was.

Brown, Andrew: I was thinking of going down the resource route to to do that protection.

Brown, Andrew: What do you think of that?

Stuart Schaefer: Umm like yeah, I it's a question of now and future state.

Stuart Schaefer: Like when I think of the current tensor definition, it grounds too hard.

Brown, Andrew: Yep, Yep.

Stuart Schaefer: If you ask, we get to a place where you have a resource which allows me to define an opaque structure that can be passed through the platform and excellent like that.

Stuart Schaefer: And if it can be made to be an asynchronous stream, even better.

Brown, Andrew: Yeah.

Stuart Schaefer: But like the person who's created it and owns it, only the only one who knows what the semantics of the thing really are.

Brown, Andrew: Yeah.

Brown, Andrew: My, my, my sense there is that we should move hard towards tensors as resources so that we can do that type of like, you know, opaque, hiding when needed.

Brown, Andrew: Yes, you're right that now now right now that is not the case to tensors are very much.

Brown, Andrew: You can poke and prod them however you want.

Brown, Andrew: Any other?

Brown, Andrew: Any other comments on this question?

Brown, Andrew: I would say hi, this is great like the.

Brown, Andrew: The way that you've described this.

Brown, Andrew: Shows like what is needed, right?

Brown, Andrew: At least in your use case and how you're doing things.

Brown, Andrew: And so it's very clear if you're able to either send that those slides to me or or put them as a PDF on the on the repo that would actually, I think be very valuable for people to understand like ohh yeah, people need this kind of thing, right?

hydai: Ohh sure.

hydai: Actually address sure my slide link in the chat.

Brown, Andrew: Cool.

hydai: Shouldn't that be posted again?

hydai: If you you are not seeing that.

Brown, Andrew: I got it.

hydai: Yeah.

Brown, Andrew: I got it now.

hydai: Yeah.

Brown, Andrew: I just haven't been looking at the chat. Cool.

hydai: Yeah, yeah, yeah, yeah.

Brown, Andrew: Uh, now that I'm looking at the chat Stewart, did you wanna mention the asynchrony and cancellation comments like do you wanna expand on that?

Brown, Andrew: Hmm.

Stuart Schaefer: And I just I mean, so I think Heidi had mentioned it here, right, which is that his need for LLM S which can be, you know, stop midstream and doing things.

Stuart Schaefer: There's no provision for it.

Stuart Schaefer: I you know, I just wanted to remind that that's possible.

Stuart Schaefer: True, but there's a lot of other AI frameworks that can't do that, right?

Stuart Schaefer: So when you really look at it, you know, again, asynchrony and cancellation are not common behaviors in, you know, in general purpose AI frameworks today.

Brown, Andrew: Yeah, that's a good point.

Brown, Andrew: Yeah, that's something.

Brown, Andrew: Go ahead.

Stuart Schaefer: That, though it's, you know, it's something that needs to change, but the it's not.

Brown, Andrew: Yeah.

Stuart Schaefer: It's not commonly there.

Stuart Schaefer: It is for open AI because of the you know the the the nature of the beast.

Stuart Schaefer: But you know, in general, if you were to just copy Lama and run it on tensor flow, there's no way to cancel it.

Brown, Andrew: Umm.

Brown, Andrew: OK.

Brown, Andrew: Thank you for that.

Brown, Andrew: So I will make sure that all those notes get uploaded to the site.

Brown, Andrew: Ayako, do you wanna talk about your agenda topic next?

Ayako: Uh, OK, let me share my screen.

Ayako: Can you see my screen?

Ayako: What's your name?

Brown, Andrew: Yes.

Ayako: Yeah.

Ayako: Sorry. Hey.

Ayako: Uh, OK, so thank you very much for allowing me to explain the our interest, to be honest.

Ayako: I also thinking maybe this is not related to Wash NN so but let me explain our interest.

Ayako: So yeah, and my request is more maybe unreasonable.

Ayako: Yeah.

Ayako: Yeah.

Ayako: So at for my understanding of what their burgeoning it initially began because it was difficult to determine the cause of fear where loading, especially when the encoding was set to auto detect, then that issue may have resolved and evolved around communicating errors effectively.

Ayako: But Stuart suggested an order on alternative solution the enumeration API.

Ayako: I'm not entirely sure where the the elimination API would work before or after Deployment II loading the air module or not, but if it is before the loading model, yeah, I think this is this API can fit our IO TH AI requirement.

Ayako: So yeah, from this uh animation API.

Ayako: Uh, I imagine that functionality is similar to the price can kind of device scanning then and like.

Ayako: But yeah, so today I want to explain our use case, but our use case that is actually not so unique this.

Ayako: I think this is a typical edge AI system.

Ayako: In it is system AI inference can run in various environment.

Ayako: For example, cloud side?

Ayako: I ask this feature set of feet fork, maybe some PC and sometimes on the device side.

Ayako: So then recently with the introduction of wasm, it was become possible to run this process even in resource constrained edge environment.

Ayako: So therefore we want to choose the optimal location while minimizing power consumption, addressing privacy concerns.

Ayako: To achieve that, we need to have prior knowledge of environment.

Ayako: Then what kind of information is required at the?

Ayako: I bring this.

Ayako: Ah yeah, here is a piece of information that came to my mind.

Ayako: I apologize this all this is also not the wronger Washington and related, but at first what she bajon I believe this information can be provided by word as a component model and supported encoding type.

Ayako: I think this is mentioned by Stuart also supported IO right emails that sound and yeah other how to access ready to accelerate the information like GPU, GPU.

Ayako: But usually we use DSP.

Ayako: This is not listed on the WASI NN yet, but it we usually use DSP so and also we wanna get our memory information before loading a model.

Ayako: Because uh device has a not enough memory sometimes so.

Ayako: That's why I interested in the discussion regarding modular balcony.

Ayako: I wanted to know the possibility of the path in this instead of detecting errors during deployment, I I want to know before the deployment.

Ayako: So I want to get enumeration API or other way.

Ayako: Maybe runtime comfy can't help this.

Ayako: I I'm not sure.

Ayako: But uh uh, my anyway, I'm very interested in this discussion because of this reason.

Ayako: Yeah, that, that's all.

Brown, Andrew: Thank you.

Brown, Andrew: That is, uh, that makes a lot of sense, actually.

Brown, Andrew: Uh, I used to work in Intel's IoT group for a while, so I'm familiar with like, the the chart you showed where it's like, oh, we wanna do things at the edge.

Brown, Andrew: No, in the middle, no in the cloud like that that similar, you know, layering of the problem like I'm pretty familiar with.

Brown, Andrew: I had a question along these lines.

Ayako: Yeah.

Brown, Andrew: You know, do you want to discover this information in band, like from inside a web assembly module?

Brown, Andrew: Or do you want to be able to discover this information out of band outside of web assembly module through some other you know?

Ayako: Here.

Ayako: Umm, when you read after.

Brown, Andrew: Anyway, I've you know right there's many APIs, right?

Ayako: OK.

Ayako: Ohh, but they are.

Ayako: If uh was can support this kind of enumeration API for each each industry, each area not all, not only one.

Ayako: CNN also watch CNN.

Ayako: I at least I want to get this kind of encoding type hardware.

Ayako: How do information?

Ayako: But if yeah, it's difficult.

Ayako: Uh, I don't mind to get the data from our outbound.

Brown, Andrew: Yeah.

Ayako: So no, not related to Wasson and but anyway I want to get them permission from, you know.

Brown, Andrew: Cause I'm sort of thinking like this.

Brown, Andrew: Like, let's take the let's take the.

Brown, Andrew: Can I print something right?

Brown, Andrew: Can I print to the console?

Brown, Andrew: That's a good.

Brown, Andrew: That's like a capability that may or may not be present on an edge device or something, I guess.

Brown, Andrew: When I E the the way with works right now is when I compile the module, I must know that there is an API coming in.

Brown, Andrew: It's like, hey, I can print right?

Ayako: Umm.

Brown, Andrew: And so it is like statically encoded at compile time that that capability will be present and if I go take that module and I run it on the embedded device and there is no way to print text.

Ayako: Umm.

Brown, Andrew: I will at instantiation get a essentially a linking error like hey, I can't do this right like like I on this runtime on this device I don't I can't run this model and so they're actually that level of static, right?

Brown, Andrew: They're they're compile time static.

Brown, Andrew: When you're talking about like text and and stuff like that, right?

Brown, Andrew: When we were talking about wheezing and model stuff, we're actually kind of thinking it of it dynamically.

Brown, Andrew: Like you know, we're running the web assembly module and then we ask it.

Brown, Andrew: Ohh, can you support Onyx?

Ayako: Yes.

Brown, Andrew: What versions of ONNX can you support and it would like return the list and then we'd load the model or whatever, and so I are you suggesting that we should do?

Brown, Andrew: What do you think about that?

Brown, Andrew: Sort of distinction.

Ayako: Yeah, as you mentioned here, uh, we cannot know.

Ayako: The functional supported cannot before deploying.

Ayako: Actually deploying to the some embedded device.

Ayako: Umm yeah, so problem is not only for the machine running umm.

Ayako: Yeah.

Ayako: But umm.

Ayako: Yeah, but if one wow API support this uh, this computer maybe other.

Ayako: That's true.

Ayako: Umm, I'm thinking.

Brown, Andrew: Yeah, yeah.

Brown, Andrew: Right.

Ayako: Yeah, I'm anyway for further ID and HI system if there is this kind of API it will be helpful because we don't want to have our error in any case.

Brown, Andrew: I I think you're identifying that the that there is a requirement here to be able to figure out what can this device run right?

Brown, Andrew: Like you need to know the information before you ship down any web assembly modules down there.

Brown, Andrew: You're like, hey, can you, like, I need to know right now, I feel, maybe, maybe this makes it more concrete.

Brown, Andrew: Like there's two ways of doing that.

Brown, Andrew: One you you make a.

Brown, Andrew: MQTT request.

Ayako: Umm.

Brown, Andrew: I don't know what protocols you guys use, but you can you make a request on some protocol to that device and say what capabilities do you support and it returns back some list of capabilities that could.

Brown, Andrew: That's like the out of Bound case.

Ayako: Mm-hmm.

Brown, Andrew: The Inband case is like this.

Ayako: Umm.

Brown, Andrew: You send down a web assembly module.

Ayako: Umm.

Brown, Andrew: The web assembly module has access to this like capability API and it's sort of like asks the system hey, what version of Washington do you support and what models and ohh hey can I print to the screen and the web assembly module itself like collects all that information and then somehow sends it back or or something.

Brown, Andrew: Right.

Brown, Andrew: So that you get the the actual correct web assembly module eventually running.

Ayako: This is.

Brown, Andrew: So you're starting to sending down a spy first that says, hey, can I?

Brown, Andrew: Like, that's the in band.

Ayako: Umm.

Brown, Andrew: Uh, model and I, I don't.

Brown, Andrew: I don't know it when I say it, it sounds more complex, but maybe.

Brown, Andrew: Maybe it's not.

Brown, Andrew: In the end, I don't know.

Ayako: Yeah, I think outbound is maybe enough. Yeah.

Ayako: So yeah, if let's make a system doesn't want to have a kind of inbound solution.

Ayako: Yeah, of course we should discover some way in the outbound.

Ayako: Or.

Brown, Andrew: Yeah, Stuart, you had your hand up.

Ayako: Yeah, but no.

Stuart Schaefer: I I I I think we have to be careful here in separating out.

Stuart Schaefer: You know, I'll call it imprecisely, structural capabilities versus soft capabilities.

Stuart Schaefer: OK.

Stuart Schaefer: In general, with the WASM system is designed to do.

Ayako: Uh.

Stuart Schaefer: Is to say I've got the following things from a security perspective that I want this interface lazy.

Stuart Schaefer: I want this.

Stuart Schaefer: You know, let's call it definition of the world.

Stuart Schaefer: Will you allow those things to run and gain access to the platform?

Stuart Schaefer: That's very different from how is the platform configured, right?

Ayako: Umm.

Stuart Schaefer: So if you think about this thing that says like, can you run it like in general?

Stuart Schaefer: That's what you usually ship down the WASM module 4 and assert capabilities and all the rest.

Stuart Schaefer: And it's just that single sort of access control mechanism.

Stuart Schaefer: Then there's this thing that says because it has Wasi and I can use Wasi and I know that it can do inferencing.

Stuart Schaefer: OK.

Stuart Schaefer: It's just a question of doesn't have a GPU and NPU doesn't have it.

Stuart Schaefer: This doesn't have it at what can it do?

Stuart Schaefer: What's its version?

Stuart Schaefer: You know, I agree that we want to solve those things all apriori.

Stuart Schaefer: But there's sort of two halves to that itself.

Stuart Schaefer: Problem right?

Stuart Schaefer: I think like I said, we just, I don't think you want to bundle together this thing that sort of interjects with is that the structural wasm kind of, you know, nature, a deep set of soft capabilities, you wanna have it be just things to say.

Stuart Schaefer: Yes, I can do as a yes, I can do a X.

Stuart Schaefer: Then when you query the platform for like, which version or whatever, that's when you want like again you want polyfills, you want things that say OK, you can only do this version of the runtime.

Stuart Schaefer: So I'll only have this sort of version of quantization and not that one it it's.

Stuart Schaefer: It's like at the platform says yes, I can run it.

Stuart Schaefer: You know, your mileage may vary.

Stuart Schaefer: You wanna go fast, right?

Stuart Schaefer: And then when you wanna build the best code that runs on an edge device, you want to say it can handle all the rest.

Stuart Schaefer: But if you bundle them together, I think it's an unbounded interface.

Stuart Schaefer: Yep.

Brown, Andrew: You know Stuart, sort of the channel, some of the discussion from the plumber, plumber summit last week and I'm not trying to endorse this idea, but uh in talking about Wes is SQL.

Stuart Schaefer: Yes.

Brown, Andrew: Umm, you know, there's naturally concerns about like, OK, the sequel and Postgres is different than the SQL in Maria DB and SQLite whatever right?

Stuart Schaefer: Yes.

Stuart Schaefer: Yes.

Brown, Andrew: And so one solution that sort of came up was, well, let's it's sort of to the, to your point about bundling all this, all the structural and sort of dynamic configuration, configuration things together, they one someone said hey, why don't we have kind of like 2 levels of wit interface the the vendor specific one which is like ohh I grabbed the Postgres SQL and version zero point 5.3 and I use that to sort of instantiate my SQL thing and then I use the generic SQL thing to do the queries that or whatever whatever the standard thing.

Brown, Andrew: Are that are the same across all the variants, right?

Brown, Andrew: Two different levels of API and then and then.

Stuart Schaefer: Sure.

Brown, Andrew: That way in the web assembly module that comes down, both capabilities are like clearly visible uh.

Stuart Schaefer: Sure, but that's, that's it.

Brown, Andrew: I do want to use a vendors thing.

Brown, Andrew: Go ahead.

Stuart Schaefer: Sure, but that's that's that's an extension model, right?

Brown, Andrew: Yep, Yep.

Stuart Schaefer: It says here is here is what the the standard is and we've provided in that standard and extension model.

Brown, Andrew: Yep.

Stuart Schaefer: OK.

Stuart Schaefer: But I'm saying something different, which is this?

Stuart Schaefer: Is that even above that level there is the base capability of web assembly itself that says if I ask for a world from a runtime OK it will assert a set of capabilities from a security perspective?

Stuart Schaefer: Will I even let you run web assembly?

Stuart Schaefer: Will I even let you run wazzy?

Stuart Schaefer: No.

Stuart Schaefer: Even though I have those called neural net stuff underneath me, I'm not going to let this instance of a runtime get access to Wasi, right?

Stuart Schaefer: So we need to do is separate out the things that says.

Ayako: No.

Stuart Schaefer: You know, I think like, hey, the first bullet here, which is, yes, I support wasi.

Ayako: Like.

Stuart Schaefer: Which version of Wasi is part of the world description and the capabilities of the runtime?

Stuart Schaefer: And then you get down to this next layer, which is, you know, OK, what can I do with that instance of wazzie?

Stuart Schaefer: Right.

Stuart Schaefer: And then I think what you're suggesting is that there's a third, which is, you know, vendor extensions.

Stuart Schaefer: Sure.

Brown, Andrew: And again, I I don't want to say I endorse that one way or another, but it was brought up in, you know, basically you're you're saying let's divide them.

Brown, Andrew: They were saying, well, we don't have to.

Stuart Schaefer: Nope it.

Brown, Andrew: We can just, you know, have the the vendor vendor specific API and the and the standard APIs all glommed together and it should be we should be.

Stuart Schaefer: No, nobody.

Ayako: The.

Stuart Schaefer: But again, it's actually it's actually divided because how do you know that you can use the vendor API?

Brown, Andrew: Right.

Brown, Andrew: No, I understand your point, so.

Stuart Schaefer: Right.

Stuart Schaefer: Which is it?

Stuart Schaefer: Like what we're trying to solve here is let's say I send down Postgres specific code to a system that's actually running my sequel.

Stuart Schaefer: It isn't gonna work.

Stuart Schaefer: Right.

Stuart Schaefer: So we need to basically say how do I query the platform and ensure that what I'm trying to do is even gonna work or that I know to send the right additional module.

Ayako: No.

Brown, Andrew: So one thing that occurs to me at yakko is that one way to figure out if a module run on an embedded device is to try to instantiate it on that device, and then you'll immediately know if it'll work or not, right you.

Brown, Andrew: You know it will it.

Brown, Andrew: It won't ever run any code if it can't instantiate and during instantiation you may be able to figure out.

Brown, Andrew: Maybe everything that you need to know, I don't know.

Brown, Andrew: That may be one way of querying it.

Brown, Andrew: Something to investigate, I don't know.

Brown, Andrew: They one else have any comments here? Question.

Brown, Andrew: All right.

Brown, Andrew: Thank you so much at Yako for sort of like bringing up this requirement.

Ayako: OK.

Brown, Andrew: I do agree with you that it might be are more.

Brown, Andrew: It's more general than just the Wasi NN.

Ayako: Yeah.

Brown, Andrew: Uh, right.

Ayako: Yeah, but we we outbound weight and if still we need some something in bound maybe I will come back again.

Brown, Andrew: Umm.

Ayako: But yeah, maybe we can start considering from outbound.

Brown, Andrew: You know, I I wonder if just opening this type of requirement on the wazee repo itself, right?

Brown, Andrew: Might not be a bad idea to get people aware of.

Brown, Andrew: Like what you're trying to do and what you're trying to solve, and to see what their opinions are right outside of just the Wasi and world like just wasi in general.

Brown, Andrew: There might be other opinions out here that I didn't represent fairly or something.

Ayako: OK, maybe I I will do that.

Ayako: Yeah.

Ayako: Thank you very much.

Brown, Andrew: Matthew we had.

Brown, Andrew: Describe improvements or tensor data validation.

Brown, Andrew: Extract applicable part of solution from bytes to scussion do you want to talk about that?

MTR: Yeah.

MTR: So one is but I have a question about that and then two is in terms of, I know it's supposed to create a ticket, but when I went on I felt like there were already 2 existing tickets that kind of covered this.

MTR: So one was one is like specified coercion of non wasn't types which is uh uh sort of idea.

MTR: Behind that one was that WOW was that you needed to be able to.

MTR: Umm.

MTR: Basically return like like in order to make it useful after you make a call to like compute and you get that tensor back, you need to be able to make it useful.

MTR: Umm into like F64F32 or whatever.

Brown, Andrew: Umm.

MTR: And there's currently like no safety around that.

MTR: You don't know what's like embedded in the tensor.

MTR: You don't know what the program actually expected.

MTR: Uh.

MTR: And so that that that's one part of it.

MTR: And then second part of it is like, OK, how do we like pass bytes and how do you embed things into bytes?

MTR: Umm and I and the and the overarching question for me was more like which part is more important for this purpose?

MTR: Like what part should we be focusing on?

MTR: I get the impression that like the.

MTR: Yeah, like the coercion of types, like being able to convert to F32 is more important one then like necessarily, like figuring out how to pack bytes just because one is like needed to really make it useful, right?

MTR: So if you do run like inference like there's a bunch of different types of like tensors that can come back off of running inference on the image, and there's a bunch of different input tensors to the different types of models for performing inference on the image, and so it's much better to nail that down.

MTR: And so I'm happy to like draft like the OK, here's the set input F64 and like all those changes, but we wanted to make sure that that's the way.

MTR: So we wanna go down.

MTR: Or do you want me to just draft it to have a look at, like, hey, this is what we wanted to look like and then use that as a like concrete thing to, like poke at.

MTR: Uh, yeah.

Brown, Andrew: I I mean, I agree with what you're saying about like, we don't need to worry about bytes as much, right?

MTR: Yes.

Brown, Andrew: Like that the the the bytes type.

MTR: Yeah.

Brown, Andrew: I think you're saying like, yeah, OK, let's cut that off for now.

Brown, Andrew: No one really needs it needs it.

Brown, Andrew: We do need to safely pass like arrays of F-30 twos and F64 and all that stuff.

Brown, Andrew: We need to be able to do that better.

Brown, Andrew: I'm like totally on board with that.

Brown, Andrew: I had sort of thought what if we do it on the?

Brown, Andrew: Like kind of as a as a as fields or as as methods on the.

Brown, Andrew: The resource the tensor resource right be like, hey, I wanna construct a tensor that is, you know, a bunch of F-30 twos and have methods for the F-30, twos for creating a tensor and and then methods for reading F-30 twos from A tensor.

MTR: Yeah.

Brown, Andrew: Uh, hanging on the research?

Brown, Andrew: What do you think about that?

MTR: So I I think that that's fine, but I think the the bigger question, right, it may not bigger question but the the question I think we're looking at is there is like if you have a, a, an F-30.

MTR: Uh, like, there's like FP16FP8.

Brown, Andrew: Yeah.

Brown, Andrew: Yeah, but.

MTR: And so like, apparently you it's and like you 16 and you 30 twos and I30 twos don't actually like exist.

MTR: They're just like utility types around like I32 and I64 or something.

MTR: Uh.

Brown, Andrew: Yeah.

Brown, Andrew: Yeah.

Brown, Andrew: What is that like?

MTR: And so the question is like when when you read those like, how do you read an FP16 like you have like that doesn't even exist in like.

Brown, Andrew: Yeah, yeah.

Brown, Andrew: Did you see that FP16 discussion?

MTR: Yeah.

Brown, Andrew: Google's trying to spin up a proposal to like add FP16 types to the.

MTR: Yeah, right.

Brown, Andrew: But.

Brown, Andrew: Yeah, yeah.

Brown, Andrew: Yeah.

MTR: And then, but there's like FPGA and FP4 and it's like, you know, like trying to like like, like playing whack a ball with, like someone figures out.

Brown, Andrew: Yeah.

MTR: Ohh I can pack this crazy thing into 2 bits and I'm sure they'll be a 2 bit neural network eventually.

Brown, Andrew: Hmm hmm.

MTR: So the question there is, umm yeah, like do like I think my my suggestion was in the ticket was hey maybe we should just we can either push the requirement on the implementers or we can like put the onus on like the consumer of the API basically say like you as a consumer are responsible for like loading some no standard uh or I I guess that's that's two rush specific you as a consumer responsible for marshalling the the tensor bytes to whatever format you want which isn't really that friendly to callers of the API.

MTR: Uh, the other one is we like basically bundle that up or solves and say like we're gonna put like for every supported tensor type we're gonna provide you like a read of this type and then a set input of this type so that you can just pass it in and underneath the covers it might still all just get turned into bytes.

Brown, Andrew: Mm-hmm.

MTR: But then we're constraining like what you can pass in to the enum tensor type, and I think another issue is right like we have that enum tensor type and so that, like suggests that we support all of those different types.

MTR: But like FP16 is on there now and and BF 16 is on there.

MTR: I know Google's working on trying to add that in, but the only way to get that is to load some extra crate, right?

Brown, Andrew: Yeah, yeah.

MTR: Uh into the system?

MTR: Like you can't even create like create that type like right out of the box.

MTR: Yet we have it as a supported input in the tensor type.

Brown, Andrew: Yeah.

Brown, Andrew: Right now we're kind of defaulting to the first thing you said, which is the consumer has to set up the the the BF16 bytes correctly and then pass those bytes on, right?

Brown, Andrew: It's a very low level API.

Brown, Andrew: Uh, what do so on the other side?

Brown, Andrew: Uh, let's say we wanted to pass in like we wanna make BF16 tensor, right?

Brown, Andrew: And we wanna like send it off to the model.

MTR: Yeah.

Brown, Andrew: What's the second?

Brown, Andrew: How would that look in the second thing you mentioned like?

MTR: Umm, I think you'd have to, uh, call, set input and then basically like I think you you'd create, you'd create the tensor.

MTR: Umm.

MTR: And I think you have to like instantiate the tensor from a set of bytes.

MTR: But the other thing you could, but sorry I said that wrong.

MTR: When you it would be nice if you could do create tensor and pass in an array of like BF16.

MTR: However, we don't.

Brown, Andrew: Yeah.

MTR: That type doesn't exist, so right now there's there's no way to to do that, right?

MTR: Like, we couldn't even do it for those types ohm.

MTR: But like it is.

Stuart Schaefer: Yeah.

MTR: Yeah.

Brown, Andrew: Well, well, maybe we could pass a list of like F-30 twos and say we're passing that 32, but we really want you to, like downscale that to be a 16 or whatever, right?

MTR: Yeah.

Brown, Andrew: Go ahead.

Stuart Schaefer: I don't think that's a great idea.

MTR: Uh, what?

Stuart Schaefer: I really just have, I mean cause the problem you have with that band is, is that you essentially say you have a a machine that has some expectations over memory.

Stuart Schaefer: And now what you're gonna do is potentially take a very, very large body of memory and allocate it in a completely opaque fashion.

Stuart Schaefer: Do a bunch of.

Stuart Schaefer: You really want that?

Stuart Schaefer: The reality is, I think what you said before is that you can, if you want, load a crate to the centrally has the ability to synthesize BF16.

Stuart Schaefer: If you're using BF16IN web assembly, yeah, you better know how to use web.

Stuart Schaefer: You know at BF16.

Stuart Schaefer: If not, don't use it.

Brown, Andrew: Yeah.

Stuart Schaefer: But like, if you're going to make an assumption under the hood that you can pass FP 32 into something that expects BF16, that means you're going to ask every framework to do that for you.

Stuart Schaefer: Like, really.

Brown, Andrew: Yeah, but it means or or the runtime has more.

Stuart Schaefer: Like I said, I.

Stuart Schaefer: Right.

Brown, Andrew: You know that code that, that create that we brought in outside would now have to live inside the runtime right to down convert.

Brown, Andrew: Right, right.

Stuart Schaefer: But you're saying, but you're saying any implementer of a wasm runtime has to now start building a bunch of code on top of these frameworks that do translation cuz no framework that I know of today does translation because they don't want to be in the business of memory management.

Stuart Schaefer: And what we do and like ONNX runtime is, you basically you create an allocator and you pass some of the allocator and say great if you wanna do some transforms.

Stuart Schaefer: Here's the allocator.

Stuart Schaefer: When you're done, I'll handle the memory correctly and delete it and free in whatever.

Stuart Schaefer: But like I don't wanna be responsible for managing the lifetime of this piece of memory.

Stuart Schaefer: It's a bad idea.

Brown, Andrew: Yeah.

Brown, Andrew: Yeah, I'm with you.

Brown, Andrew: I think what I do here and what Matthew is saying is that well, but that what about UX?

Brown, Andrew: What about DX right?

Brown, Andrew: How do we how do we make it easy?

Brown, Andrew: Easier for developers to get those BF sixteens there.

MTR: Yes, Sir.

Brown, Andrew: Like you know, there's some mythical crate out there that potentially does this, but like, right, is that sort of, you know what you're talking about, Matthew?

MTR: So it's it's merely just like right now.

MTR: You basically have to serialize into bytes and get it right.

MTR: Umm.

MTR: And and to and to show and to give you an example of like where this gets kind of funky, we in the one of the example creates for like the image inference.

MTR: Uh, there's like a specific image that like, actually just crashes the like, open VINO model when you like and it's a, it's a picture of an owl.

MTR: I don't know exactly why it does it, but there's no way for me.

Brown, Andrew: Yeah, yeah.

MTR: It's kind of take these color channels, map them into a tensor and somewhere like there's like a off by one hour and then just like and then just crashes.

MTR: And so it's like really hard to introspect and maybe this is more around like getting like passing errors through the thing but.

MTR: The it's.

MTR: It's also like a not always clear like exactly.

MTR: I mean, there's a lot of marshalling steps right in between and so you can either say, OK, you have to know everything and this was, this is also the part of the point of taking.

MTR: And I definitely see Stuart's point, which is like you don't want to like, you don't know what the model expects.

MTR: And this is kind of like opaque to the model.

MTR: So like sorry, you have to do the memory management and serialization yourself, but that's also gonna make it like a lot harder for folks.

MTR: And it's a lot more error prone.

MTR: Uh.

MTR: Even if you know what you're doing to just like make a mistake in the serialization and I've even thought whether like buy order matters.

MTR: Umm.

MTR: Right.

Brown, Andrew: Yeah.

MTR: Like if let's say you encode like uh, I mean it won't, it won't matter.

MTR: And like like we say, assume like, hey everything is is.

Stuart Schaefer: But but but but but think my it it does right?

Brown, Andrew: Mm-hmm.

Stuart Schaefer: So if you if today imagine the following, OK, if you have a tensor that expects any CCWM and you pass NCWC, you get the wrong answer, right?

Stuart Schaefer: Abject failure, right?

MTR: Yeah.

Stuart Schaefer: What's hard is that there's all these weird places where you have to know how to use the model that you're trying to accomplish.

Stuart Schaefer: You need to know enough about the metadata to do the right thing, and what I'm saying is like solving that problem.

MTR: Yeah.

Stuart Schaefer: I think personally goes way beyond the Charter of Wasi NN the Hum and I'm not sure I really want to put in an embed code and force runtimes to do stuff to solve that problem because it's a big one.

Brown, Andrew: Hmm.

MTR: Yeah.

MTR: And I was thinking, even just like byte order for like the numbers that are gonna get loaded into the CPU, right?

Brown, Andrew: Umm.

MTR: Because this is stuff going over the network and then uh eventually like you serialize into bytes, then you could definitely like just like if you know that the underlying system right is.

MTR: Uh, I'm a little endian versus big endian.

MTR: I mean, I doubt there's any really big endian systems that that are people are running these models on, but in theory someone could go run out of a big endian system and you like, run the exact same wasm and then it would just like barf because you encoded the bytes manually.

MTR: And then when the system actually went to execute it, it blows up.

MTR: So yeah, I I.

Brown, Andrew: Yeah.

MTR: But again, I agree with you like it does seem that like the IT current answer and best answer seems to be like as much as we'd like to make it nicer, it is very dangerous to make it nicer in a uniform way, or at least that there's no like good way to do it where we ourselves are wouldn't be tackling these huge like problems for which there are no satisfactory answers at the moment. Umm.

Stuart Schaefer: Yeah.

Stuart Schaefer: Yeah, I I gotta.

Stuart Schaefer: I'd asked the the question which is like we put BF16 into the interface.

Stuart Schaefer: Can it even be done?

Stuart Schaefer: Right.

Stuart Schaefer: Can you construct a BF16 tensor and web assembly?

Stuart Schaefer: If you can't, it shouldn't be an interface.

Brown, Andrew: Hmm.

Stuart Schaefer: If it's painful, that's a different question, right, you know.

Brown, Andrew: Hey.

Stuart Schaefer: But that's what I'm saying.

Stuart Schaefer: You can exactly.

Stuart Schaefer: You can.

MTR: I mean, you can always construct it like bit by bit and there's a there's no standard create that you need to do it, but again you're passing points.

Stuart Schaefer: It's pain.

Brown, Andrew: Painful, yeah.

Stuart Schaefer: You, you, you, you can and it's pain, right?

Stuart Schaefer: But like that, I think for us to unwind, that pain is is is 10 times bigger of a problem.

Brown, Andrew: Yeah.

Brown, Andrew: Hey, can we?

MTR: Well, I I mean.

Brown, Andrew: We're two minutes over.

Brown, Andrew: Hey so I think we should continue this discussion.

Brown, Andrew: I think there's some tension here between the how easy do we make this to use and are we trying to boil the ocean?

Brown, Andrew: Can we do this?

Brown, Andrew: Matthew, you're.

Brown, Andrew: You're gonna kind of either take one of the issues and convert it or OK, that one you wanna use that one.

Brown, Andrew: How about?

Brown, Andrew: OK.

MTR: Yeah, it already basically already describes the everything that we just discussed and and I and just a quick the the counterpoint is, hey, we listed these things in the enum tensor type already.

Brown, Andrew: Right.

MTR: So we already took, we already said BF 16 and the interface, and if we're including it in the interface, you could make the argument that you should like everything that we include in that enum.

Brown, Andrew: Right.

MTR: We should have a method for, right?

MTR: It could be like, hey, you do an FP8 model, but we don't have that in enum.

Brown, Andrew: Yeah, yeah.

MTR: That's like, OK, you're on your own, but if we list it that were already sort of taking stuff like making a statement that we know what this is and we're gonna pass this metadata through and maybe this framework should be smarter about it.

MTR: That's just playing devil's advocate, but that's all I had to if you want to.

Brown, Andrew: No, no, that's that's a great point.

Brown, Andrew: And now I think, how about we continue this discussion in the next worker meeting, like I'll put this as the first agenda item, I this issue, I'll try to sort of summarize some of these questions and then let's come back and like see if we can umm, I think there might be something around metadata that might like help this.

Brown, Andrew: There might be something around libraries, I don't know if.

Brown, Andrew: Let's try to think of, umm, ways to resolve the tension because I I actually agree on both sides, right?

MTR: Yeah.

Brown, Andrew: Like we need to make it easier to use as much as we can, yet we can't.

Brown, Andrew: We can't boil the ocean, right?

Brown, Andrew: We can't do everything.

Brown, Andrew: It would be insane. So.

MTR: Yeah, that's why I didn't.

Brown, Andrew: OK.

MTR: I like started looking at this and I was like, I don't think I should draft anything for this yet because it's not clear clear that like, yeah, the direction wasn't quite clear.

MTR: That's why I wanted to come back to the group first.

MTR: OK.

MTR: Thank you.

Brown, Andrew: That's good.

Brown, Andrew: OK, so first agenda item.

Brown, Andrew: Next time will be this.

Brown, Andrew: Umm.

Brown, Andrew: If you have any other agenda items or anything that you like discussed, let's talk about it actually to Rob has something about like FP8.

Brown, Andrew: So maybe I should ping him about that.

Brown, Andrew: But thank you everyone for attending.

Brown, Andrew: I'll send out the notes later.

Brown, Andrew: Talk to you guys in two weeks.
