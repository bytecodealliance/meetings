# Wasm+ML Working Group &mdash; March 18

See the [instructions](../README.md) for details on how to attend.

### Agenda

1. Opening, welcome and roll call
    1. Please help take notes &mdash; thanks!
1. Announcements
    1. _Submit a PR to add your announcement here_
1. Proposals and discussions
    1. Discuss WIT proposals for tensor residence, copy avoidance, streams, etc. Matthew-Tamayo
       Rios, Stuart Schaefer, David Justice (30 minutes, [context])
    1. Discuss how to communicate tensor layout, e.g., NCHW vs NHWC ([#67])
    1. Discuss adding the GGML encoding format ([#66])

[context]: ./ML-03-04.md
[#67]: https://github.com/WebAssembly/wasi-nn/issues/67
[#66]: https://github.com/WebAssembly/wasi-nn/issues/66

### Attendees

- Andrew Brown
- Stuart Schaefer
- David Justice
- Steve Schoettler
- Matthew Tamayo-Rios

### Notes

A high-level summary:
- GGML ([#66]): David Justice to request more details in the issue (is it
  [this](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) format?)
- Tensor layout metadata ([#67]): Andrew Brown to summarize in the issue: probably better to have
  graph-level metadata (not tensors); wasi-nn is a system-level API and there is little it can do to
  be completely developer-friendly; developers do need to understand a model's requirements,
  metadata can only superficially help with this.
- WIT proposals:
  + David Justice presented a streams-based addition
    ([diff](https://github.com/WebAssembly/wasi-nn/compare/main...devigned:wasi-nn:stream))
  + Matthew Tamayo-Rios presented a tensor residence addition
    ([diff](https://github.com/WebAssembly/wasi-nn/pull/69/files))
  + Andrew Brown presented a residence + tensor creation addition
    ([diff](https://github.com/WebAssembly/wasi-nn/compare/main...abrown:wasi-nn-spec:residence-proposal))
  + Steve Schoettler suggested we keep tensor, "the data structure," separate from a tensor
    "resource"; Steve plans to create an issue and present the use case at the next meeting
- Some TODOs: Andrew and Matthew plan to merge their proposals; David is going to create a wasi-nn
versioning scheme so we can move forward with 0.3 planning; Stuart was asked about graph encoding
versioning &mdash; may need this before GGML addition (?).

What follows is the Teams transcript of the meeting:

-----

Andrew Brown: So I added a couple items to the agenda. This in the last two weeks, 2 new
issues have come up UH-1 about, you know, additional metadata on tensors. Umm and the other on, you
know, a new encoding format. So let's talk about the new encoding format. I don't know how much you
guys do with GPML, but it seems like a reasonable to me suggestion to allow it as an encoding format
for graphs. The uh issue is right here. Umm. And so I kind of, you know, I I think I've talked to
Dan in the past and he's interested in this stuff. I know other people have used G ML as an encoding
format. Uh, but they just never thought it necessary to add it upstream into Wasi. And then as like
an encoding format. So what do you guys think? Any thoughts on GGML?

Steve Schoettler: Well, I am. I'm not familiar with GGML, but we've back when I when I first started
using. The tensor I was also using Seymour and I noticed that they had a a really well thought out
format for encoding tensors that also that also had a richer metadata. And I think and. And. In in
my original PR in the in the repo I had a link to it, but yeah, I think I mean high level. I think
we should learn from what's been done and if they're, if they're standards that have been around and
evolved for a while, then we should seriously look at them.

Andrew Brown: That makes sense. I think you cut out there midway through Steve. I think I got what
you mean though, that like Steve Bork could be useful for understanding how to encode tensors. I
think though this GGML issue is talking about encoding graphs, not the tensors themselves. Alright.
Any other comments on GGML as in new back end kind of encoding format? Alright. Well, I'll, I'll
take a look at it this week. I am inclined to give this a thumbs up, so unless anyone else has a
serious objections, umm we can leave this item as is. Ohh David, what's up?

David Justice: I quick clarification so this is the I posted a link GGUF is that the specific file
format that like this port?

Andrew Brown: Umm. I don't know. He doesn't. He doesn't give a lot of details in his. And his thing
I I think that I I'm not sure it would be the the file format. I think it would be maybe the in
memory format. But. Yeah, there's probably some relation there. Do you want to follow up that in
that issue and ask for more details?

David Justice: Yeah, I don't. I don't have a whole lot of background to DML, but happy to ask some
questions like a naive observer.

Andrew Brown: Yeah, that would be good. Maybe more details good. Umm OK. The another issue that came
up was uh, uh how are we gonna encode additional metadata on tensors? And So what this is about is
that someone was trying to distinguish between, you know, NCHW and NHWC or something like that.
Here's the issue. They don't have really a way of of like doing that with the wheezing and interface
right now. And so they wanna to maybe add extra enum somewhere so that they could, like, uh, you
know, specify the tensor layout. I think this is kind of dangerous because if you I mean the NHWC
stuff is very image specific and so if if for every domain that someone trying to create tensors, we
have to also create, you know enums for distinguishing between the different ways that those tensors
can be laid out. If we kind of enter into an interminable spec. Uh hamster. Wheel of trying to keep
up with with the latest uh, you know, you know different way different metadata that people could
like attach to tensor. So I'm a little bit cautious about this one. What do you guys do it?

Stuart Schaefer: My experience here is Ben is that you attach the metadata to the model and the
expectation is of the consumer to provide the tensor and the right format.

Andrew Brown: Yeah.

Stuart Schaefer: It's rare that what you wanna do is excuse me inside of a model, be flip flopping
back between those going swizzling, right? Because in general, like we do that right? Because what's
hard here is this. OK, so like and the GPUs prefer one layout format as opposed to, let's call it
Intel GPUs. And so sometimes internally the model will do those switches under its control, but it
generally says that the boundary. This is what I expect, so model metadata says this is the tensor
layout I expect. If not like it could put in a a a head, but it doesn't. Right.

Andrew Brown: Yeah, that's.

Stuart Schaefer: You're asking somebody. Someone who implements the spec to add code that's not
there.

Andrew Brown: Yep. You brought up a good distinction, though. I think it is true that if we did do
something like this, it might be better to do it at the like graph level than at the tensor level.

Matthew: If they're loading it, they shouldn't. Shouldn't they have access to that information
anyway? I like like I I just I I think I just agree with that. I think that was that was Stewart,
right?

Stuart Schaefer: Yep.

Andrew Brown: Umm.

Matthew: Uh, yeah. But basically, the biggest assumption here is you have to know how your model
works and what inputs it takes, and then how to generate those inputs in wasm. And so if you know
all these things, then like query the.

Stuart Schaefer: Right.

Matthew: Sorry.

Stuart Schaefer: And tooling can and tooling can also navigate the graph metadata right? Like the
whole point of putting it on the graph is that you can auto generate code and all these other kind
of things based on the graph metadata. If you leave it wide open, that's where all becomes a
problem.

Andrew Brown: So should we? So maybe let's shift over to graph metadata. I kind of agree that it
would. The graph is a better place for this kind of metadata to live. Uh. Should we have a way to
maybe like Matthew saying like query the graph and say like hey what what metadata is associated
with this graph somehow?

Matthew: I'm. I'm actually saying that I don't even think like that.

Andrew Brown: They shouldn't have. Yeah.

Matthew: That also would it makes yeah cause basically like that means every time some new graph
property or some new graph thing gets introduced. The way the sort of criteria I would say is like
they would have to be like some compelling thing for interacting with that metadata from wasm where
like they there's like some major piece of like the common functionality missing that they just
can't implement without access to that metadata and to right now for the most part is if you are
loading a model you you have to go get that model from somewhere you have to know what is in that
model and you have to be able to prepare tensors as input to that model and just. Generally
speaking, like if you know all these things right, like in what case would you end up with a model
that like has the a different layout than what you're expecting?

Andrew Brown: Umm.

Matthew: And so that the only way I see that's like you're running two models with two different
layouts and you are somehow choosing which layout at runtime and then you are like choosing which
type of tensor to generate. Like, I just don't seem like kind of.

Andrew Brown: Yeah.

Matthew: I I don't see the the use case to sort of motivate it yet I could be wrong. Like if I hear
something, I fully reserve the right to do. Like uh, no, that makes sense.

Andrew Brown: So like Stuart, I think in the past you've talked more about uh metadata and that
there were some things that we'd want to be able to control from the WebAssembly side on the model.
All umm.

Stuart Schaefer: Yes, model.

Andrew Brown: And and and you want the and I think if I remember correctly that metadata you didn't
want it to be baked into the laziness spec. It was kind of like key value pair stuff, right?

Stuart Schaefer: Yes, yes, yes. But yeah, what? What? What the distinction I've been trying to make
is is I think what you know, Matthew was hinting at is that to be clear to people that Wasi are not
developer time APIs. Right.

Andrew Brown: Yeah.

Stuart Schaefer: They're not instrumentation and things like this that you use frameworks and other
things for right that you know, help you to kind of navigate these, let's call it, you know, issues
of how do you build and define code. They're high performance platform system APIs, right? If you
don't know what you're doing, is Wasi supposed to help you?

Andrew Brown: Yeah.

Stuart Schaefer: Like really fix all that I I don't know. That's not what POSIX is. You know what I
mean? Yeah, much too big, right?

Andrew Brown: Yeah, it's too big of an ask of Wasi to yeah, yeah.

Stuart Schaefer: But but like I said, it's it should be pretty straightforward to say like look,
there are some very specific properties that are important about certain classes of metadata to just
expose it. If something wants to expose it. Right. And I had argued before that essentially when you
look at it, there is certain I'll call it metadata about, you know, about graphs. That's just it's
really simple as is and key value pairs to put stuff out and if someone wants to look at it, look at
and. If you don't you don't.

Andrew Brown: That's kind of what I was thinking is if if we were gonna add like an interface for
setting metadata on the model like key value pairs to control execution in some way, maybe we'd also
have a way of like retrieving some of those key value pairs and maybe one of those key value pairs
could be like, you know, for models that you know are related to this stuff, right to to image
processing of some kind.

Stuart Schaefer: OK.

Andrew Brown: There could be a way of retrieving which kind of what's what's the order of the
dimensions or whatever.

Stuart Schaefer: Yeah, like that.

Andrew Brown: Yep.

Stuart Schaefer: If if you look at the Onyx spec, you'll see exactly this he ONNX model spec has a
set of predefined metadata and that it has a set of key value pairs that let you put in whatever you
want and and to be fair, in a lot of code in the Microsoft platforms we put stuff into that metadata
to add benefit for tooling, not runtime tooling.

Andrew Brown: Umm.

Stuart Schaefer: Right. So if you use tools like netron and stuff like that to open up and look at
models, you can see some of this stuff and we use it to guide code generation and other things like
that.

Andrew Brown: Yeah.

Stuart Schaefer: But like run like at runtime system spec never touch it, never touch it.

Andrew Brown: Yeah, I I think even, yeah, I think going back to the Matthew's point, even if we did
have a way of retrieving a key value pairs off of a model, and even if you could sort of see, uh, if
there were a way to, like, distinguish between HW C and then NCHW or whatever, that would kind of be
a fragile way to do this. And and really like you, you'd already have to know that the model is, you
know, is what you expect it to be right before you even ask that question. Uh. Through the metadata,
so I think we're kind of in, you know, this would offer such minimal benefit to a developer, right.
It wouldn't really offer the kind of benefit that this user's asking for, which is like, hey, can I
really easily figure this out? And it's the answer probably gonna be like, no, even if we add this
metadata, you're still not going to have the developer friendly experience that you might expect.
You're still gonna have to have kind of baked in assumptions about the model that you like
retrieved.

Stuart Schaefer: Yeah, I think it's the question is this, is that do we believe that our caught
development time things with models are polyfills, right? Or that the WebAssembly API is essentially
a catch all for everything to do with the neural net, cause like I said, if you just look at like
platform APIs like and and API and things like that, none of this is there.

Andrew Brown: Yeah. Yeah. OK, I think we're all kind of on the same page then about this. I can try
to formulate a response and I'll like point back to these notes and. Yeah, I I think the the I, I
guess this is kind of the response I was thinking, Yep, I think at some point we'd like to
investigate metadata more likely on models than on tensors. But yeah, there might be a way to do key
value pairs on models. But I but Mr whoever this is, I'm sorry. That's probably not gonna be enough
for you and. So you know. I think this should be like a higher at at a higher level library you
might be able to do this type of like figure this stuff out. Alright, if anyone else wants to
comment on this issue, absolutely feel free, but I'll try to give this user. Some type of response.
Umm, OK. The other item on the agenda is talking about let's. Brainstorm some proposals for how
we're going to keep tensors resident on certain devices. Avoid copies. Figure out streams and stuff
like that. David, you have a suggestion, do you? Should we start with that and then let's go through
any others that people have?

David Justice: Umm, we can go through. Do you want me to open it up? Sharing screen or some?

Andrew Brown: I I can do it if you want. I think I have it up here.

David Justice: It's it's very small. So basically it's just a streaming of it in our previous
conversation, we had talked a little bit about Wasi IO and streams for tensors so that we didn't
have to materialize the entire. Uh, you ate list? Uh, so this is a possible. Version of it using
streaming which would be available in preview free. So tensor data becomes a constructor taking type
of resource that takes a stream as a constructor Arg. Basically just contains the data and then
eventually has a finish where the stream is red to completion and as long as there's no children. I
I wasn't exactly sure about the stream child is still alive. That's really hand WAVY and was just
basically copied over from the HDPA stream side. So ohh.

Andrew Brown: So they have a finish over there.

David Justice: And yeah, yeah, they do.

Andrew Brown: OK.

David Justice: So if you if you I I think I call out in the link here where this where this you know
the inspiration for this came from?

Andrew Brown: Umm.

David Justice: Yeah, there there's the link. OK. So yeah, you can take a look there. Really it
doesn't change much. The great thing about this as opposed to the preview 2 like Stream or Wesley IO
implementation is that in preview tier you still have. Yeah, basically an input and an output, and
you can't really share the type. You can't share the stream from an input to an output. They're
they're two distinctly different types, and it it really makes the interface look pretty yucky, so I
think this is a a good way to show it. Anybody have any questions about that?

Andrew Brown: And I saw Stuart, you had your hand up earlier. Was that about this or a differently?

Stuart Schaefer: I. No, I took it down. It was. It was.

Andrew Brown: OK.

Stuart Schaefer: It was hanging out there for stuff.

Andrew Brown: But you you didn't have some comments about this earlier?

Stuart Schaefer: I I did, I think I think like I, I completely understand it. I I had just wondered
the question of is the necessity to implement stream a high bar?

Andrew Brown: Like it's not gonna be really available till Preview 3. Is that OK?

Stuart Schaefer: When a no no, don't care, but don't care what the available part, what I care about
is. This is. Let's just say that someone writing a pretty simple app that wants to take, you know,
some data from something and run a simple like snapshot of some UI element and wants to run some
segmentation OCR ish. Whatever kind of thing on it and basically wants to take that memory and pass
it into here, what do they have to do to turn it from again?

Andrew Brown: Hmm.

Stuart Schaefer: A simple memory region into a stream. Is it a high bar? Does someone has to pass?

David Justice: Yeah.

Andrew Brown: Umm yeah, that's a good point.

Stuart Schaefer: Like it can cause streaming is great for. Let's call it the advanced scenarios.
Right.

Andrew Brown: Yeah.

Stuart Schaefer: And I love it. I'm just wondering to make sure like is there a quick way for
someone to get a stream from a simple memory region?

Andrew Brown: Yeah. Because you know, if you're getting it from a file, you probably get a stream
like. Great.

Stuart Schaefer: Yep.

Andrew Brown: OK, I already have the stream right?

Stuart Schaefer: Sure.

Andrew Brown: But yeah, when when I'm manually constructed something or yeah, now that's a good
point.

Stuart Schaefer: But like it but like both in both in both Java and C sharp, there's like, you know,
memory streams, right?

Andrew Brown: Yeah.

David Justice: Yes.

Stuart Schaefer: There's basically the ability to take some memory region, create a stream. Is that
they're in wasm. Is that going to be there and the no, I know, I know. Doesn't saying is like unless
there's some easy way to get to stream right?

Andrew Brown: Yeah, yeah.

Stuart Schaefer: Are we going to create some pain?

David Justice: I think it has to.

Stuart Schaefer: Has to, doesn't it?

David Justice: So when I think about I I think it absolutely has to like you, you have to have this
kind of conversion or these these niceties. Abby streams proliferate. I think you're going to have
to have like translation from. Like OK, I just have a list you eight. OK, let me translate that
directly to a stream like the simple case or or I have a you know basically that's just a memory
buffer, right? Ohm and yeah, I can't imagine like for HTTP then I can't imagine that there aren't
gonna be niceties to say. OK, I have this Jason that I serialized the data and now I need to send it
on the wire right?

Stuart Schaefer: Yeah, it's insane.

David Justice: And if you tell me, like I have to go figure out how to construct a stream every
single time, you're basically telling me to go # sand, right?

Andrew Brown: Umm.

Stuart Schaefer: Yeah, it's insane. Yeah. And like that, that's my concern, right? It's like it's
like it's like if you've created this scenario where I have to write like 100 lines of code every
time I want to touch whatever like that's kind of silly, right? And what we usually done is with
APIs like this kind of said like there's two APIs. There's a streaming API and a memory API. They,
but if if it's a trivial rap to get a stream, fine.

Andrew Brown: Yeah. Well, I I think, Davide right that like for the for for these for Wasi
interfaces that use streams to be useful. All these conversions need to be like super easy to do
right? And what then? I start thinking about is, you know, so you're creating the system interface
was is the system interface, but then you sort of have to attach it alongside a little standard
library of functions that kind of like do these kinds of things for you, right. And we're where is
that standard library gonna be, you know? Or are we gonna have to rebuild that standard library in
the Rust standard library and see? And whatever right like. Because I I yeah. I don't think it makes
sense for he pulled to have to reimplement stream versions of very various kinds themselves. That's
like not. It's just a nonstarter, right? Yeah, but then where does? Where does the code live then
you know? But I I think I think what we should do with this is like note that if we go down this
route, we need to make sure that that dependency is like met.

Matthew: Yeah, there's also a lot of fun stuff about streams being a highly overloaded word. Uh,
like a stream in rust?

Andrew Brown: Yeah.

Matthew: And then like a, it means it means a lot of things. Uh versus, say like a A like a
streaming C sharp like they have like very semantics and who knows what the final semantics of this
is going to be. Uh, I I will say really quick after jump off. The I also put the second part of
we're talking about and it's a complete or thogal to the discussion around streams was the ability
to control the memory residency of the tensors.

Andrew Brown: Mm-hmm. Umm.

Matthew: So like basically be like even if you can use a stream to initialize it and the stream to
get it out, the separate question is if you want to be able to control loading it onto the CPU and
GPU or TPU for example, we need functions to do that. I did a very small draft PR that I linked uh,
that basically adds it in. I basically followed the same pattern as Pytorch which.

Andrew Brown: Umm.

Matthew: Basically as CPU, GPU and then any any fancier things, I use the existing execution target
enum and yeah, and I also made data return a result tensor data with based off the discussion we had
last week where if we if you try to get the data from a GPU tensor. Like you want it to fail.
Obviously, if we if we change it and say instead we wanted to return the stream, that data function
gives just become a stream. And so if you try to get data off of GPU tensor, you would just to
handle all that stuff underneath the covers to turn it into a stream that comes back to Wasi and
then uh, but it actually ended up working out cause like this is like sort of these are separate
decisions from the stuff that needs to be decided for stream. Uh, and it's pretty straightforward.
It's not. It's basically like just when you run this you will get a new tensor and that new tensor
will have done called the underlying framework to like move it to the location.

Andrew Brown: Umm, I actually took a stab at this. A similar see, but I I factored this I I did the
same kind of idea but I factored it a little bit differently. Let me show you this. So I said, well,
maybe you can specify the execution target. Maybe when you create a tensor and then you can you can
ask it what you where you at. And then I say whenever you try to get the data then then it's gonna
do a copy or whatever, right. Uh. And so I think that it, uh, what what? This is what I yeah, I feel
like it's kind of the same idea as what you're thinking, Matthew.

Matthew: Yeah.

Andrew Brown: But I I kind of uh, parameterize it on execution target so that if we added more
execution targets or whatever in the future, we don't have to add new methods, but it's the kind of
the same thing. The two, yeah.

Matthew: Yeah, I I that's why I created a generic one, but I specified the those two specific ones
that just because like 80, probably more percent of the use cases are literally going to be CPU and
GPU as the execution target. Uh. And so like making those that's Pytorch does the same thing. If you
look at it what those function calls in pie torch, if you call dot technically they say CUDA instead
of GPU.

Andrew Brown: Umm.

Matthew: But what they actually do is they just call 2 device CUDA underneath.

Andrew Brown: Yeah. Yeah, yeah.

Matthew: They like just a wrapper for that to make it easier because there's so frequently invoked
cuz like if you need to like this matters more in Python, but if you need to do like a plot then you
need to like convert it from CUDA back to a CPU tensor so that you can you can plot it. I do.

Andrew Brown: But, but you know, go ahead.

Matthew: The other thing is about location. Target is that it always starts in the CPU because you
always have to load it into CPU memory and you have to pin it and you have to move it. You can in
Pytorch, tell it to default to GPU, but all it does is like loaded onto CPU memory. Copy it over and
then drop the CPU one.

Andrew Brown: Mm-hmm.

Matthew: Uh, so having that whether that location gets set at construction time or it is wrapped by
doing A .2, it basically just delays that all that you're controlling is when the thing gets dropped
from memory, whether it happens automatically or not.

Andrew Brown: Yeah.

Matthew: Uh, you're not you. You're never actually. You're never doing anything like literally
streaming the bytes onto the GPU.

Andrew Brown: Yeah. Yeah, yeah, yeah. Yep.

Matthew: It's always like pin memory copy over over and over again.

Andrew Brown: Hey, but the the one thing I realized is that we can't just parameterize on execution.
Star I I took another stab at OK create on device right. And actually I think we need to, I think
godly, let me do this. OK, I think we need to it's on two things, right. We we actually need to, you
know, if you're if you're just using pie charge, then you can just say ohh I wanna not CPU or GPU
and Pytorch knows both. You know, it knows what that means, but at the wasi and then level, we
actually don't know really which backend we're talking about, right. And so I think we might need to
also. Kind of say, hey, you know I I'm using Pytorch and I want it to be on the GPU, right? Like we
need both things. Because I assume there's there's differences in how Pi torch wants to get things
to a GPU and how it keeps track of that, then ONNX or open VINO or any of these things. What do you
guys think about that?

Matthew: I mean, I feel like that is a much bigger question than A and then what we're talking about
here, right, because if what you're saying is these tensors are not resources that are floating
around forever and we don't know what framework created the tensor, then that's a much that's a.
That's a much bigger question and I just I'm off the top of my head. I don't remember from like
whether you could have multiple frameworks like in in a single instance.

Andrew Brown: I mean the API kind of supports that and so that's why I'm kind of like maybe
someone's gonna do that someday like. And that that's why.

Matthew: Yeah.

Andrew Brown: Kind of think we might need to attach. We might need to tell it like ohh. This is
you're you're doing pie torch tensors right now, right? And you can't use a pie torch tensor with an
ONNX tensor or something like that.

Matthew: Yeah. And I I feel like this also goes back to a previous discussion we had about like
graphing codings and frameworks are also don't mean the same thing, right?

Andrew Brown: Yeah, yeah, yeah, yeah.

Matthew: Like you could have an on an X graphing coding executed by open VINO.

Andrew Brown: Yeah.

Matthew: And so like.

Andrew Brown: Yeah, really. I I didn't want this to say graph and coding I think. I I want it to be
a back end or whatever some other concept right? But this is what we had, so I I went with it.

Matthew: Yeah.

Andrew Brown: But you're right. I I think we do need to keep those things kind of separate.

Matthew: Yeah.

Andrew Brown: Yeah.

Matthew: Making sure that it's compatible with the graph. Otherwise, things would just like it. If
it will be very difficult for developers if they are mixing tensors in an environment to make sure
that they keep the tensors sort of like straight of like.

Andrew Brown: Yeah. Yeah. Yeah.

Matthew: Oh, I created this one with the on the next back end and I created this as a Pytorch tensor
and now I have like 2 tensor resources that I'm keeping track of, and then I'm going to make sure I
remember which framework and so then when I call compute, I have to make sure I can't accidentally
pass the wrong one. Maybe that's a reasonable thing for a small number, but I suspect we're more
complicated programs that could get people into trouble. And I feel like Stuart should have strong
opinions here.

Stuart Schaefer: I I was trying not to. I'll call it. You were on A roll and and I didn't agree to.
I didn't disagree with anything right. It's it is very dangerous to mix mix frameworks right? Just
because something's on a GPU tensor in Pytorch and you move it to tensor flow and it's still a GPU
tensor like you know, be careful right? Which allocator created it?

Andrew Brown: But yeah, yeah.

Stuart Schaefer: Which one thinks they owned it right? It's super dangerous stuff.

Andrew Brown: Yep.

Matthew: Yes.

Andrew Brown: Yeah.

Matthew: Yes.

Stuart Schaefer: What? What? What? But, but let me let me make up.

Andrew Brown: So it it feels like we kind of need somewhere of talking about which which backend the
tensor is like attached to, but I even feel like it should be at a better level than this, right?

Stuart Schaefer: But you have a rewind, right? If you think this way like in in ONNX runtime,
there's the notion of a session. OK. Intense reflow. There's the notion of a session only. Pytorch
doesn't do this because eager mode is like let you do whatever you want and you can kind of go
crazy. But like the moment you open up a session, you've created a bunch of fixed things, right? And
you essentially said that like, this is how the world's gonna work. And if you want to change, you
gotta be explicit and create another session, right?

Andrew Brown: Yeah, yeah.

Stuart Schaefer: Was dangerous as to not do that when under the hood we have these sessions full
stateful things, right?

Andrew Brown: Yeah, maybe, maybe.

Stuart Schaefer: And we're not talking about High Torch eager mode here. We're talking about like
frameworks that do allocations and create memory pointers and like, do things that have state.

Andrew Brown: It's kind of like maybe the way to create tensors should be hanging off of graph
execution contexts, which is kind of like the session you're describing right? Cuz at that point we
do know what backend we're talking about. We do know what model we're talking about and if if a
graphics execution context were creating the the the tensor right, like if if that was the
interaction point at which we're like, OK, here's the bytes for tensor, we would know. You know,
we'd already know this. So like, that's out of the window and maybe we could do more checking on
this, right? Like uh, we might not even need to specify the location, because at that point maybe
we've already said, hey, this this session is happening on the GPU. So just make sure the tensor
data gets put on the GPU, right? So there's a I is that kind of what you're getting at? There's a
different way of looking at it, like a session based.

Matthew: Yeah.

Stuart Schaefer: Yeah, I mean, we don't necessarily have to make this session explicit, right?

Andrew Brown: Yeah.

Stuart Schaefer: But like, if you look at in the API right it it's it's this thing that says once
you've sort of created certain resources like you're committed, right, you're you're you're, you
know, you're looking at the graph. That graph has an encoding. That graph has a back end. It's been
allocated, you know.

Andrew Brown: Umm.

Matthew: Yeah.

Andrew Brown: Matthew, you're gonna say.

Matthew: And I was gonna say is that the location you still need location cause you there are cases
where you would want to do a get a CPU tensor even though you're like graph execution context that
the tensor is associated with is GPU like right? Like let's say you ran your uh classification
algorithm and and output. It's like 1000 element vector and now you need to like sort that vector
and like do operations on it.

Andrew Brown: Umm.

Matthew: You're gonna want to do like CPU and then read it out from the GPU in some way. I guess you
could say like if you always can get like a tensor data which is just like the list you ate then and
that's going to do a copy, that would probably be good enough as long as you don't need to do you
never expect to have CPU model in a GPU model where you're passing data back and forth. But again,
all these things is like we start making assumptions about the architecture. Uh, I think that like a
that the safest thing would be to you always have to. You always need to be able to control the
memory residency because you won't be able to predict the architecture and residency of the graph of
neural networks that the end user will want to work with. And then separately from that we I think
your suggestion to attach them to the graph execution context scenes evidently reasonable. Uh.
Because then at least you're binding them to some back end that's associated with that context. I
like that. Seems again, that seems eminently reasonable at first.

Andrew Brown: Hmm. Umm.

Matthew: First glance, I just think that we will still have to deal with location and we're really
just using that binding to bind the tensor to a specific.

Andrew Brown: Umm.

Matthew: Ohh, back end uh, or session of some sort. So that's just my could take of it.

Andrew Brown: Any other take there? Umm, OK, so we have a couple proposals. Stuart, did you have a a
something you wanted to share or anything?

Stuart Schaefer: I did not. I was. I was hoping that the folks who are deeper JavaScript and was
called Wasm Masters than I would propose something that I could critique. I can I just like I did
not.

Andrew Brown: I I think I think at this point we got the ideas on the table like I think David
brought brought the streaming bits in and like, that's probably like how we would have to do
streaming. Uh, stream is not here yet. We do need to make sure that the any helpers for like take
this memory block and make it a stream. Those need to work. So that's that aspect now, E Matthew and
I are kind of dealing with this residency location stuff. I think I think we're clear that we need
something like that and I I kind of like your your two function. Here. Just it's just it's very
clear, right? That's like, hey, I wanna move this to this device. Like that kind of thing. Uh, but
then? But then yeah, I think we kind of opened up another option which we haven't really written
out, which is like what if we created tensors on a session? How do you how do you guys wanna go
forward with like crafting this? Right. I can try to put everything together into one thing but. We
could all just. Maybe I could just list out like the different options and we can like. What do you
guys think? How should we decide? Yeah.

Matthew: We should do the minimal changes.

Andrew Brown: Yeah, yeah, yeah.

Matthew: I, but I've I've just looking at that create on device function. I'm just like, oh, that's.

Andrew Brown: Yeah. So a lot, yeah.

Matthew: So I I think that the. Yeah, I guess there there's two decision points. One is like uh, do
we want uh, do you want the location function? I do you want to be able to query the execution
target? I think that's probably that's an oversight. Leaving that out in my part.

Andrew Brown: This is the.

Matthew: So I would say we want that we want the two.

Andrew Brown: This is the read version. Yours has the right version, right? Like.

Matthew: Yeah. Yeah.

Andrew Brown: Yep. Yeah.

Matthew: So like basically just merge those two and then delete create device on device here and
then basically in graphics execution context like have the create tensor.

Andrew Brown: Yeah. Yeah.

Matthew: The question is do does that mean that the tensor constructor goes away because we never
want the tensor to just be constructed like freely?

Andrew Brown: Yeah. We kind of want it. Then we wanna limit it to being built on the.

Matthew: I believe it would be in graph execution contexts.

Andrew Brown: Yeah, I'm kinda. I think that makes sense to me. Let. Maybe I'll draw that up and we
can like mold that over cause like that's a big change kind of. I don't want to screw it up.

Steve Schoettler: Can I ask a question?

Andrew Brown: OK, Steve, go for it then, David.

Steve Schoettler: Yeah, I'm. I'm not sure if if I'm if I'm understanding this, but. Do we still have
the property where a I could construct a tensor without knowing what kind of engine it's going to
run on?

Matthew: I don't think we can.

Andrew Brown: So we're we're just discussing like, well, maybe maybe we shouldn't, right if if we if
if the only way to create a tensors on the execution context then you wouldn't be able to just
create tensors.

Matthew: I well, I was going further to say I don't think we can because any any create tensor
function right like we we are wasi-nn never actually makes its own objects effectively right? We
have no, no independent notion of a tensor, so if someone calls create tensor underneath the covers,
it's gonna be forwarded to like π torch tensor or tensor flow tensor, or like, like TF2 dot tensor
or whatever. Like, we're never going to like create an array and be like this is your own tensor
class implementation that is like shared among the. I mean, I guess we could, but that is not
something we have done right now.

Steve Schoettler: Well.

Andrew Brown: Right, right.

Matthew: When you create a tensor 8 that's forwarded to the back end.

Andrew Brown: Umm.

Steve Schoettler: So I'm I'm what if you want to what if you wanna have the ability to let's say
have a mobile app that streaming data to a server and you wanna benchmark different back ends or you
wanna see well, do I really need a GPU or can I do this on a CPU and you want the freedom to be able
to change back ends and not have to also change your client app?

Matthew: I mean the way you would write it is right, based on the proposal that we we just
described, you would call on your graph execution context. You you would have to call basically
create tensor right? Cause of your mobile back end is gonna have to serialize that tensor, so it's
gonna have to get the data, serialize it into either JSON like number array or if it's using a
binary format serialize it into like a binary format floats, send it over the wire on the back end.
You're gonna have to deserialize that, and then you have to put it back into like the tensor with
the correct back end. So I don't see it actually stopping you from switching back ends. If you
wanted to even run multiple backends, it's actually possible now, because you would create four
graph execution context and then you would like you could actually just do like create tensor and
then take the existing tensor and like get it's tensor data and pass it to the next one. If you
really wanted to, I imagine you probably actually just deserialize into a back of like F32 or
whatever or F16, and then you would pass it into each graph execution context on the back end. But I
don't see any reason why you couldn't do what you described in a fairly straightforward fashion.

Andrew Brown: I agree with that, but I think he's what what I hear is Steve is like you're splitting
the difference between tensor the data struct and tensor the the actual resource that might be on a
GPU and stuff like that, right? Like there, you're kind of saying, well, there might be value to
just having the data structure right, that that encodes what dimensions are there and the the item
type and the maybe even the bytes and that data struct is kind of like useful maybe even for you
know if you're just passing around generic things that are kind of tensors, right. If you actually
wanna do computation, you need the you need tensor at the resource, right, not tensor the data
struct. But maybe there's a difference there, and maybe it's useful to have both.

Steve Schoettler: Good.

Andrew Brown: It's kind of is that kind of where you're going, Steve?

Steve Schoettler: Yeah, you're you're right that I'm that I'm referring more to the the tensor, the
data structure and I wanna be able to have the client create that in in a compact format that
describes the the multidimensional array and whether yet you you of course have to decide whether
you want to use F-30 twos or in store or whatever the data types are and the dimensions of the array
that's part of the API that you would establish between your front end and your and your back end.
And and I would hope that through through duck typing or whatever, effectively you can decide is
that close enough?

Andrew Brown: Umm. Mm-hmm.

Steve Schoettler: Is that the right structure to send directly to my model like my Unix model or do
I need to do some conversion and it'd be nice if if in some cases you can avoid conversion as
opposed to forcing a conversion every time because it's not compatible with any back end.

Matthew: I mean, I don't you have to convert every time anyway. If you're doing like if you're using
Python, when you call create like create tensor, you have to pass in like a Python array and then it
look and then the framework then actually does the conversion.

Steve Schoettler: Well.

Matthew: Uh, I I will. There are some exceptions, right?

Steve Schoettler: Yeah, I'm.

Matthew: If you do like dot zeros or dot ones or something like that to generate like but those are.
Right.

Steve Schoettler: It might be talking about creating an array in in web assembly using the
WebAssembly API, the Wasi-nn. API to create a tensor. They're not not calling a function create
tensor, but just generating the array. Maybe it came from a A image conversion function or
something?

Matthew: Yeah. Yeah. So I I guess what I was trying to set set up the analogy of like if you are,
uh, like you, you you have some array of numbers, right? So like that, that's a tensor data struct,
and you're saying you're trying to avoid, like copying it to be able to use it in the framework. And
what I'm trying to illustrate is that, like, hey, even like the to use it in any framework, right?
Like whether it's wasi or or wasi-nn, or whether you're directly working in the frameworks
themselves, there's always this step of I have some like VEC like numeric data type that I then have
to pass into a create tensor function to make it useful. There's there's no like framework that will
let you take like a a raw array and I should I should sit be I'm trying to be very careful here
because there are things you can do like you can like create like a raw and DRA and then like, raw
reinterpret that into like a but you you can do some like funky things. But like the normal path is
not is not like create a raw and DRA and then like use that to read it into a tensor. It's normally
like here's like zeros or ones, or here's like my big array of numbers. Create tensor from this big
array of numbers and then do computation on it. And what I'm saying is that whether you're in was in
the world or whether you're in like the native language from the framework, you always end up having
that step to sort of load the tensor and make it useful. And the sort of precursor steps to that are
always just basically an array of numbers of some type and you have to know. And so like I can see
it being useful, but I just want to make sure that that we're not saying that we're gonna avoid a
copy.

Andrew Brown: Mm-hmm.

Matthew: We're just saying we have a useful representation for that, like precursor step inside of
Wasi and I'm and that's why I guess I'm saying it could be useful to have it like as Andrew said,
but it's not going to save a copy, which is what I heard.

Andrew Brown: The desire.

Matthew: And one of your comments, it's simply going to make it easier to have a have a thing to
pass around.

Steve Schoettler: Yeah, it's it's been a, a a few months since I wrote the code, but it it did
actually use the ND array so I I was. At least had had to have an awareness of the Indian Ness and
the the the way the numbers were packed into the array so that I could pass it right to an ONNX
model. Umm and uh, and but but that tensor is was not specific to Onyx, it was just the memory. But
but I don't wanna.

Andrew Brown: Yeah, that's a good point.

Steve Schoettler: I don't wanna derail the conversation if if if my context is, is is out of date.
Umm and I don't.

Andrew Brown: No, but I I think you have a good point, Steve. I I think I think tensor data struck
tensor resource. Maybe we need to split, but we probably need to discuss that a little bit more. Can
you open an issue about that right and and maybe providing more context from your uh, from the
experiments you did would be like helpful because maybe we do actually need the split. But you know,
we we definitely I think we've been moving towards resources this whole time. So anyways, I I think
we could probably discuss that more. I just wanna make sure that David has a chance to get to that.
Yeah. Dave has been waiting with his hand out like.

Steve Schoettler: Yeah. OK. Go ahead, Dave, give it.

David Justice: Ohh thanks Germaine. To this topic area, we should probably create a WIT, Dash, Niro,
3.0. Draft and start because there's there's likely going to be some significant API changes.

Andrew Brown: Yeah, yeah, yeah.

David Justice: Do we wanna land those in like the .3 break everything?

Andrew Brown: Yeah, yeah, yeah. Now that's a good point. So you're saying we should? We should
version what we have now before we make any giant changes and then so that we can get to a new
version. What? What's the what's the version you just said? I there's a lot of dashes in there. Can
you say it again? Yeah. OK.

David Justice: So following the the Wasi HTTP example, they're doing just another directory with
like WIT Dash 0 point, 3.0 dash, draft E and I.

Andrew Brown: OK.

David Justice: I think that I think that's probably a a good way to just like, hey, you can do a
quick compare between these worlds and and files and and see the differences and we can yeah
advance.

Andrew Brown: OK, I I think I I'm in favor of this. I think what we should do though, is we don't
even have a I don't like what's the current version of our. Her uh. I don't even know if we really.
Yeah, we haven't even versioned like the current version of our wit.

David Justice: Yeah, we.

Andrew Brown: So first steps first, let's get a version on this and uh. OK, alright. So that's
that's a good point. We're coming up five minutes from the end of the meeting. I think there's a
couple of things that we need to maybe like decide on for next time. Steve, if you want to like it
created an issue for the struct use case, right? And maybe bring it to the next meeting. We could
probably talk about that more in depth and I think it's gonna touch on the how developer friendly do
we wanna be versus how closely and performance critical stuff do we need to like really take care
of? Are you cool with that, Steve?

Steve Schoettler: Yes.

Andrew Brown: Yeah.

Matthew: That I'm actually super interested in seeing, like how cause I feel like in order to make
that really work we would have to have some and DRA like Bing in wasi-nn and that in and of itself
seems like an incredibly tough technical challenge to to make something like that work in rust.

Andrew Brown: Right. Uh yeah. OK. So that that's one thing. I think this whole versioning and like
how we're gonna move to 0.3. David, do you wanna like? Go for it. OK. And then Matthew? You and I
probably need to like merge our like residency stuff together into one kind of proposal.

Matthew: Yeah.

Andrew Brown: How about we work on that in the next couple weeks and then like 2 weeks from now we
can like present like, hey, here's what we're thinking for that.

Matthew: Sounds great.

Andrew Brown: So that sounds good. OK, I did start out by talking about GGML. Still, no ones really
opposed to G ML like I guess we David, can you ask for more details like let's just make sure, let's
just make sure there's more details there. But I I think you know you know how we can't really
exclude GL if we're including, you know, all these other encoding format. So, umm, alright. Anything
else?

David Justice: You a quick question.

Andrew Brown: Any go for it?

David Justice: Quick question, uh. What backends are should support that encoding format?

Andrew Brown: So I think what this guy dance probably doing is he's running llama CPP somewhere.

David Justice: Yeah.

Andrew Brown: Yeah. There there we go. And so he probably has a way of running llama CPP. It's
probably not public, but you know this would give him a way of like, you know, saying what he needs
to say at the wasian and level.

David Justice: It'll be interesting if we can get it back end port.

Andrew Brown: Right. So maybe that's a.

David Justice: Like, what's the point of adding the email until we have a back end that actually
executes it?

Andrew Brown: That's a really fair question. Maybe you should like put that in the issue too, yeah.

Matthew: Yeah. Yeah, but I I feel like the biggest issue for that back end is just going to be a
doing all the bindings to call it from rust. Because it's, it's already all in like C++, so it's
not. And also it seems to be actual 74 table or as portable as intrinsic optimized C++ with a bunch
of like AVX and hey intrinsics in it.

Andrew Brown: Can be, yeah.

Matthew: Yeah.

Stuart Schaefer: And stuff copied from everywhere on the Internet. They possibly could and billed in
one piece of C++.

Matthew: Yeah. Optimized for Apple silicon, but on Etsy 86 architectures.

Stuart Schaefer: Well, uh-huh. Big. Big.

Matthew: But on PC-4 architecture something like sheet.

Andrew Brown: What?

Stuart Schaefer: Exactly so. So I put a note in Andrew. One of the things be careful here is this is
that in general as let's call it, enumerations become.

Andrew Brown: Yeah.

Stuart Schaefer: I'll call it moving targets, meaning the next 25 iterations of GGML you wanna be
super clear about saying this is the Wasi V1 or whatever Wasi ex. And if it's a constantly ever
moving enum like, don't make it an enum, make it a string.

Andrew Brown: Mm-hmm.

Stuart Schaefer: You mean like you gotta have some way to say. How do I when I query this thing, I'm
gonna know that it's gonna give me a tensor back end that I deal with, right? It's just you're gonna
have to figure out how to be super clear about that.

Andrew Brown: Yeah. And and we haven't yet split out like like, it's not just Onyx, right? It's Onyx
plus some other stuff that actually identifies the encoding.

Stuart Schaefer: Really.

Andrew Brown: Right. Or like that's what it should be. I think from comments that you made in the
past, right? Like there's different opset encodings in Onyx and right now we don't have a way of
distinguishing that at the wasi-nn and level.

Stuart Schaefer: We we don't, we don't. We don't have ONNX versioning.

Andrew Brown: Right.

Stuart Schaefer: We don't say Onyx V-15, we say Onyx, and so we have no way of knowing which Onyx is
the back end.

Andrew Brown: That. Yeah. And so I kind of didn't we talk about this a while back that we need more
there, right?

Stuart Schaefer: Yep, we do.

Andrew Brown: We need some more, yeah.

Stuart Schaefer: OK, this is this is where things become complicated.

Andrew Brown: Have you had? Do you have a clear vision in your mind of like what that should look
like? Because maybe if we we're kind of at that point where we like need it right? Umm.

Stuart Schaefer: OK, I I believe was it like 2 meetings ago someone made a proposal? Should I
revived that?

Andrew Brown: Sure, something.

Stuart Schaefer: OK. All right, let's get something, OK.

Andrew Brown: Well, let's get something here because uh, and and in the meantime, we'll like like
David will like version what we currently have, you know, and then we'll dial sort of be the
baseline. And like start moving from there. OK. Well, 1001 umm, thank you guys. I will put a. See
you later.

Matthew: Yes.

Andrew Brown: See you guys soon.

