# Wasm+ML Working Group &mdash; July 22

See the [instructions](../README.md) for details on how to attend.

### Agenda

1. Opening, welcome and roll call
    1. Please help take notes &mdash; thanks!
1. Announcements
    1. _Submit a PR to add your announcement here_
1. Proposals and discussions
    1. Present new machine learning API for Fastly Compute@Edge; Matthew Tamayo Rios (20 min)
    1. Propose replacing `load` with `load-by-name`, [#74]; Andrew Brown (10 min)

[#74]: https://github.com/WebAssembly/WASI-nn/pull/74

### Attendees

- Matthew-Tamayo Rios
- David Justice
- Hung-Ying Tai (hydai)
- Andrew Brown
- Mingqiu Sun
- Dan
- Johnnie Birch

### Notes

Here is a high-level summary of what was discussed.
- first, Matthew described Fastly's plan for caching OpenAI queries in their FaaS platform (see
  transcript below)
- then, Andrew presented [#74], a plan to remove the current version of the `load` function; all
  attendees in the room are users of `load-by-name` so there was no opposition, but we decided to
  wait to collect more feedback in the PR

What follows is the Microsoft Teams transcript of the meeting:

Matthew: Alright, I only got 1 slide for slide. Pretty sure this has been shared Let's see how this
slide show. OK, that's going to pop this up Uh where? Share share. Share share. Share share. Are you
kidding me? One second.

Matthew: Sorry, it was. I mean you start because I gave screen control permissions in Alright. Can
you guys see that?

Andrew Brown: Yep.

Dan: Yep.

Matthew: All right. So I will give a little bit of On like what exactly we're doing being related to
WASI-nn. So what the the the fastly AI Uh, it's basically a semantic leverages the fastly cache and
Auto transparently allows folks accelerator URL for talking to just be able to use it with the by
only changing a single But it it automatically helps Semantically, cache. On on without having to do
any So you don't have to do any Oh, I'm gonna set up a vector DB use like the Azure Semantic that.
It just you just point your transparently. Streaming mode, non streaming And so that the idea here
is LM's are are not the fastest back to now. Taking like 3 to 15 seconds for of that there really
expensive might ask a question of like how And you might get back like 1000 question. So even simple
questions may end bill really quickly. And so these end up being sort making a lot of sense to do as
we're good at, which is making Umm in this case L Lanza, which topic of late. So that just gonna go
over the is is basically. We use a we we basically embed similarity search and we is a super high
level. There's a lot of other pieces around making it secure and and maybe in the future and then
Like I said, one line of code so and you just pass the fastly using. Uh, basically, you can get Umm
yeah. And so the the, I think that comes in is that there's a lot that could be made faster. So like
if you imagine like this Ah, the the vector DB and the basically all the time because executed in
every request like and you have to do a vector having the ability to go native those requests. And I
think it's, I would my check the numbers. My guess would be like 80% of a those two things. Umm.

Andrew Brown: Are those are those done within Are those done natively on some

Matthew: At the moment they are, uh, but I can't go into too much

Andrew Brown: Gotcha. OK.

Matthew: me that one in the bit so.

Andrew Brown: Yeah.

Matthew: Cool. That's pretty much it. So that's what the fast It's basically a semantic cache And
and and. Yeah, I think there's, uh, we're Like can we use WASI-nn to make slash cheaper cause it
also is

Andrew Brown: Umm.

Matthew: for the product.

Andrew Brown: OK, before we. Uh, you know, maybe we can drop can talk about more private But before
I do that, uh, I had been talking recently about interface to WASI-nn that That would kind of like
address like is you'd send in a prompt know, a string. Uh. And so that and that that would Interface
vaguely defined,

Matthew: Yeah.

Andrew Brown: Do you see? Like if we added that kind of I. Is that gonna help you, or do component
pieces instead?

Matthew: So part of this is this is a opinionated position on LLM's if even if you look at the
talking about the ones designed latency is is just so great that to run an alarm at the edge like
like you could all. It's like, OK, yeah, we saved response took four seconds. Like OK, like and and
and that Like the smaller models might

Andrew Brown: Mm-hmm.

Matthew: second might still be like too right? Like it all depends on like how are they using deep
speed? Like what optimizations are they so so I'm I think we don't have I know that there are other
letting people run like some of stuff inside, like basically

Andrew Brown: Umm.

Matthew: at the edge. But at least for us, we don't right? Like it's. I'm not saying what ever
happen, time, uh, you're you're like and Amazon and stuff like that

Andrew Brown: Yeah, yeah.

Matthew: And and that that's like a can who can can operate at that on like H1 hundreds.

Andrew Brown: Yeah. No, that makes sense. OK, that's good. Good feedback. What about? OK, so how how
do you do? You want to describe now how in lazy and then that would help Or do you wanna do that?
Not on the record.

Matthew: Uh, so I'll do part of that and project that I recently did.

Andrew Brown: Umm.

Matthew: So I recently did a a quick was like can we run models at by name without going through is
it possible? So it turns out that there's I think it's like Wisener.

Andrew Brown: Mm-hmm.

Matthew: I don't know if you guys have, basically you can like do like expose this in it function
and and basically initialize the I'm talking about Rust can you can initialize something memory so
that it gets baked and this tool will basically module, run that, and then back out so that when you
like, redoing the tire thing. Yes, that one Wizer. Umm. And so my interesting thought it's called
tracked like Sonos Pytorch and like. Like tensor flow and a couple of What's the other one, the the

Andrew Brown: Umm.

Johnnie L Birch Jr: That's ONNX.

Matthew: I'm blanking on it. The one that Microsoft loves on Thank you. I was like, it's like tip of
the Yeah. So it's supports, supports those Pytorch model because it was we're just I can go grab the
of stuff to figure out how to Just I took an existing demo of Mel demo which already runs The
problem is that tensor flow module was literally too big. It was like 200 megabyte when
optimizations. But when I switched over to basically web assembly module it shaved off like 50% of
the So basically when you run it Wizer, it's literally takes Umm. And so, I mean, that's cool
literally so there's an that's happening just to get And you can basically pre bake into like your
web assembly Here's the catch though.

Andrew Brown: Yeah.

Matthew: I then tried using WASI and then so it was about 1.5 seconds. Without Wizer, it was about
700 It's been sitting there for a accelerator was consuming all of I felt a little bit more free
request through and that would actually able to like offload And there's some other stuff
transcription. That's really neat, but.

Andrew Brown: So. So basically you're saying like through via WASI NN to calculate

Matthew: Yeah. So I'll get, I'll give an example. I'm not saying that this is what at it and then
vidia in early like state of the art embedding three on like the MTB And I think it's like a like a
over a billion parameters. Umm. So yeah, if you wanna run that at the edge.

Andrew Brown: Yeah, yeah.

Matthew: Uh, right? Umm.

Andrew Brown: Yeah. Yeah, you don't wanna deal with

Matthew: Yeah.

Andrew Brown: Umm, OK. Does anyone else have any

Mingqiu Sun: Yeah. Yeah, Matthew.

Andrew Brown: go for it?

Mingqiu Sun: So I have a couple questions. So. So the first one is that the you large language
model. Do you have a need for like a in your environment?

