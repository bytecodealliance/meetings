# Wasm+ML Working Group &mdash; March 4

See the [instructions](../README.md) for details on how to attend.

### Agenda

1. Opening, welcome and roll call
    1. Please help take notes &mdash; thanks!
1. Announcements
    1. _Submit a PR to add your announcement here_
1. Proposals and discussions
    1. Continue discussion on scope of wasi-nn re: types; Matthew-Tamayo Rios, Stuart Schaefer (30
       minutes, [issue][mtr-issue])

[mtr-issue]: https://github.com/WebAssembly/wasi-nn/issues/62

### Attendees

- Andrew Brown
- Justin Janes
- Stuart Schaefer
- Matthew Tamayo-Rios
- Johnnie Birch
- Mingqiu Sun

### Notes

The main topic of discussion was on issue [#62][mtr-issue], which has to do with tensor types. Some
key topics:
- key differences between CPU- and GPU-resident tensors; need to support both
- described need for both high-performance control AND a developer-friendly API
- difficulty of keeping `tensor-type` up to date AND supporting it in Wasm
- a need for streaming tensors (e.g., wasi-io, streams)

We agreed to return next meeting with some concrete WIT proposals for supporting this. What follows
is a machine-generated transcript of the meeting:

-----

Andrew Brown: The last meeting which was cancelled, it was on President's Day and the only topic was
to discuss the scope of types. So Matthew, last time brought up a a question like what should we do
with types? It corresponds with this issue right here and. Stuart, I I see haven't fully read your
email in depth. I glanced at it. Umm, but maybe that's what we can discuss today. Perfect.

Stuart Schaefer: Alright, well, let's get started.

Andrew Brown: All of us are here. So we should, we should get started with the meeting. If Matthew
can join, then he'll join Umm so the the main. The main question here is what to do about tensor
types. So when we're passing, you know, uh data into across the boundary, to the washing and back
end. Right now we have a, you know, like a enum that says, uh, you know what type those bytes will
be. And so they're kind of reinterpreted on the host side as those bytes and Matthew question is
like, well, you know, we don't have FP16 on the wasm side, right? We may have FP16 on the on the
host side, and so we're sort of passing bytes over and then there's like, kind of this unsafe
translation. He has several issues like with that kind of thing. Umm. And so if you look at the
issue we currently have like FP16FP32FP64, we can also pass in BF16U, right? And then I 32 and I 64
now there's many other types and sort of Matthew brought that up. But what should we do about this?
Does anyone have strong opinions on whether this should change or remain the same? Stuart, maybe you
want to summarize sort of your position from the email. Sure.

Stuart Schaefer: In in general, there's sort of two different driving forces for how you define a
tensor with any framework that exists. One is to try and have it be as precise as possible, and that
generally is used for tooling and developer stories and kind of download my model from hugging face
and put it into XYZ and have it work really easily right? And make sure that things kind of line up
and do the right thing. Then there's the opposite side, which is the people that want to have the
highest performance, most control possible way of doing the things that they want and don't want to
have to Marshall and coerce and copy and do a lot of these things in the name of developer, those
called beauty, efficiency, et cetera. And so those two stories generally have slightly different
requirements and sometimes slightly different problems. My experience has been is that having a set
of defined tensor types for the former is useful, and having an opaque reference style data type for
the 2nd is. Is is also useful, right? So what we typically do is provide some way for a developer
framework to have some form of memory that they control and pass a reference to that memory and then
let it be opaque. But the developer absolutely knows that like they're in charge of any mistakes
that they make in that area. Right? And then like and then a strongly data type thing for everybody
else. So what I proposed in my email, Andrew was essentially saying how do we do something like a,
you know, uh, a variant type that essentially says I have a variant 1 variant of it. Is this
strongly defined tensor thing? The other variant is a list of bytes and the other variant is some
resource. What's the resource? TBD, but let's just call it a resource for now, right? And what that
gives us is the flexibility to kind of again have strongly type things for when we want, right, have
a clue, spite definition if you want it. I'd actually say personally I don't love it, but we want to
keep it great and then a resource for everything else, which in my mind a resources that reference
pointer. So that was like, that was the heart of my proposal. Makes sense?

Andrew Brown: Matthew looks like you joined. Yep.

Matthew: Hey. Hey, man, we were talking about your issue, I don't know if you could see in the chat
issue 62, the one about types.

Andrew Brown: So we're kind of discussing that at the moment. So I don't. Yeah, I think I kind of,
uh, yeah, right. So the the idea would be to have you know there there's one path where you can
define. Yeah, I want this to be FP16 or whatever, and then there's another path where like, hey, I'm
just gonna pass some like random bytes across the boundary and then, you know, when it comes to the
resource part of that, I think both of those actually can become resources, right? How we've defined
the width is that, uh, uh. You know those tensors get made into tensor resources that then you know
the component model can like essentially keep the data. The component model, you know if we use
resources correctly, I should say this way if we use resources correctly, we shouldn't have to be
passing around the entire chunk of memory, we're just passing around that resource handle, right? So
whether that is a strongly typed tensor or it's like the bytes or et cetera, right? All those things
can be component model resources. I think you probably meant resources in a different sense, but
like component model, resources can be these like handles that we use to like pass around instead
of, you know, copying the bytes every time. What do you think about that, Stuart? Uh.

Stuart Schaefer: So like I I think that just making everything a resource adds an abstraction for
the basic simple cases. Like I said, for developer and developer efficiency and tools harder. Right.
So as long as you have a way to say that's not the case, I'm I'm listening. Right. I'm just saying,
like, the way I kind of thought of it was, is that if you, if you use something like a variant that
says if someone passes me this truly concrete FP 32 thing, I'll wire end to end all the everything
right. But if instead they say it's a resource like, I'm just gonna pass it through. But if
everything's a resource, not I gotta crack open the resource. Go figure it out. Like, go figure it
out. Right. And so last, I'm just wondering like maybe I am, I don't understand how component model
resources are supposed to be used, but I fear that too much abstraction part, right? Umm OK, I I get
your high level concept but I.

Andrew Brown: Uh, I wonder if some of the component model details are like getting in the way here
of the stuff, right? So like can we like look at this? Let's look at this real quick. This is what
we currently have specked so uh, and this change was made in the last couple months based on some
other discussions. So we have a a. You're looking at component model ideal code right now actually.
Are you looking at it? Can you guys see this this? Yes.

Matthew: I can see that.

David Justice: OK.

Andrew Brown: And so here, here's that penser type enum that we're talking about and what the change
that got made a bit ago is that we moved to making tensors, component model resources and what that
means is that once you know, once this is a resource, the tensor can, uh, you know, the component
model has better handling for how to deal with this thing. Now check this out. Stuart, this this
type tensor data is just a list of U8, so it's just bytes, right? It's just raw and that just
essentially gets passed in in the constructor of the tensor. And that's what you know, that's the
actual contents of the tensor, right? So there's no, there's no additional, you know, abstraction
really on the actual contents of the tensor. It's just go ahead. My but my question my question was,
is that by being a list of U8 right, it means that it had to have been already marshalled into
memory in, you know something which is directly addressable in web assembly, right?

Stuart Schaefer: Yeah.

Andrew Brown: Yes.

Matthew: As opposed to a resource which is a reference handle itself, right?

Stuart Schaefer: And saying this thing that's inside of it is platform native. But in WebAssembly
right now, I think that's like kind of.

Andrew Brown: Kind of what you have to do. I'm.

Stuart Schaefer: You have to lay things out in web assembly, linear memory and then you can make a
resource out of that.

Andrew Brown: I'm I'm not sure if that's the case.

Stuart Schaefer: Yeah.

Mingqiu Sun: So.

Andrew Brown: That's what I'm I'm I'm I question that to that.

Stuart Schaefer: So you know, I I thought that that, you know, you might need to put the tensor in
the host environment as well, right for high performance processing.

Mingqiu Sun: Uh, so you may represent that the resource later on, but the initially you you know
there is a passing over data from the guest system to the host, right? That that's the step, right?
You cannot avoid. I'm.

Matthew: I'm sorry. I'm a I'm a little confused here at. Sounds like people are saying this the same
thing like the. When you, I guess, or I think the only difference between you getting passing the
list you ate from the guest to host versus passing the tensor from the guest to host is that the the
it's been wrapped as a resource and at the end of the day the API still has to go in and grab those
bytes and then actually like load that tensor into whatever the particular underlying framework is.
Umm so like I don't. I don't see that much of a difference there and what I'm hearing Stewart say,
is that there is a way that you could actually create, like the tensor may come from somewhere else,
already wrapped as a resource. Uh. And like from the host side, somehow if I'm understanding it
correctly, Stuart and you wanna make sure that like that scenario is not broken. Umm.

Andrew Brown: It right.

Stuart Schaefer: So if you look right now, there are folks looking at, you know, Web, GPU and and a
whole other kinds of things by which you take a resource right, which is memory on a GPU and you
pass essentially a a reference to that thing. Yep.

Andrew Brown: Right, rather than saying I have to take that thing off of the GPU, Marshall it
directly into memory that has it directly addressable, you know, address inside of the wasm VM,
right?

Stuart Schaefer: Yeah, yeah, yeah.

Andrew Brown: And I can and allow that external ref to be something known by the platform.

Stuart Schaefer: Yeah.

Andrew Brown: OK, so I think yeah, I get not to those like helpful. I think what you're saying is we
need a way to avoid the the copy and we'll get to you, David. One second, we need to avoid the copy,
right? If we in the use case you just talked about, let's say we have web, GPU, something or other.
There's some data that lives somewhere else we could pass in A resource tensor right into the
WebAssembly guest, right? And when we pass that in, we're just passing in the handle, and as long as
the user doesn't go say hey, give me all the data right then there has been no actual copying of
buffers. Uh, so I I feel like maybe we have preserved the use case you're talking about, which is
like making sure that we avoid the copies when they're unnecessary. Uh. Uh, so I will say give with
the one that's Stuart, just said, uh, that one is a little bit trickier in the sense that it's all
gonna come down to like the details of the implementation underneath the covers. Right.

Matthew: Because if you have if, if it's like, he's basically saying it's like the equivalent of a
Python or sorry, a Pytorch tensor where you can have the IT be like a CPU or GPU tensor and if it's
a GPU tensor then it's like living on the GPU and it's not in like main memory. And so if you're
tensor data is a list, you 8, what does that really mean? For that list you 8 if that list you 8
never really existed. Do we need a different definition of the tensor type to be compatible with
that? It's almost like you need the data to be like a resort. Exactly.

Stuart Schaefer: Exactly. Gap.

Andrew Brown: Yes, yes.

Stuart Schaefer: Another resource which may wrap a list you A instead of just being a list, you it
needs to be like tensor data is a resource and then that resource has like within the.

Matthew: You end up still with the same question of how do you define that constructor to take data
that like? Yeah, OK. Dated.

Stuart Schaefer: David, David, back to over to David.

Andrew Brown: He's had his hand up. Alright, so this is a really good one.

David Justice: So what about Wesley IO? So when we start looking at it as like a reader, writer,
sinker, reader, writer, seek, or just reader seeker, basically you have a resource that you can read
from I at various, you know, offsets and you can seek offsets right? Ohm it's basically IO. It's
basically like WASI IO, it's just a stream and but. And you're saying, why don't we use that as one
of the variants here for tensors or something like that?

Andrew Brown: Yeah, right, right.

David Justice: So basically, if you can construct uh, whatever the the stream reader needs to be
that actually implements the interface. And then you know the whoever is creating the tensor can
provide that as input and then behind it you could do any of the, you know, reading from wherever
you actually really want to to derive those fights. And the use case there is you could just open a
file and it becomes a tensor, right?

Andrew Brown: Yeah, that's an easy slab of memory.

David Justice: Or and the. The idea is it's not materialized yet, and you could probably read in and
you know grab chunks or I don't know. Does that make sense? It it does, it's it's the hard part is
this, is that I I I I agree with that somewhat and I am worried about one other thing which is this
is that if you take Matthew scenario it's exactly the one that I'm I'm sort of specifying which is
if you go to any framework there is a difference between a GPU tensor, a CPU tensor or an NPU
tensor.

Stuart Schaefer: And in general, you tend to use platform specific APIs to do things to the GPU
tensor, which is in every framework in Pytorch, and Onyx and tensor flow. There's either wrappers
around the underlying CUDA stream or Rock M stream, or whatever it might be. I just. I just check
and make sure we don't have to require another implementation to like have to implement the WASI IO
for this tensor when the reality is for most wasi needs, we're just gonna pass it straight through.
Most of that work is going to be done either above us or below us. So so how do we do the
convergence that you get the what the IO and you want to push the data to GPU.

Mingqiu Sun: So do you have like a handle two different resource and you have to convert them?
Right.

Stuart Schaefer: Us so I can answer for what happens in Pytorch and you you literally just write
like GPU like if you dot GPU and it literally copies it onto the GPU and the return of that function
is a completely separate different tensor where the data is now lives in the GPU and the copy has
been done so and so then the eventually the first tensor gets deallocated when it goes out of scope,
blah blah blah.

Matthew: Right.

Mingqiu Sun: But like, it's literally and so the.

Matthew: So how how do we do that English spec here.

Mingqiu Sun: And and bright but and you specify explicitly GPU so you don't get the the tensor copy.

Stuart Schaefer: You don't naively copy it off the GPU, manipulate it, and then put it back on.
Yeah.

Matthew: Yeah. Yeah.

Mingqiu Sun: So the question to that. Right. And and and also there's a default at the beginning of
your program that you have to specify as well, where you basically specify the default location for
tensors.

Matthew: Yeah.

Mingqiu Sun: So, should we like reflect this in the spec? You know, usually like the the tensor
source in somewhere else and you need to, you know, push to the whole thing environment for wasi-nn,
right. Do we should we comprehend that in the spec? So The thing is I think for because of the
nature of Wasi and then what we we're technically saying is that or default is always a CPU tensor,
right?

Matthew: Umm.

Mingqiu Sun: Because you're always starting with Wasi linear memory in theory and always taking some
data source inside of that Wasi linear memory and then creating your kincer.

Matthew: And then if you want to copy it to your GPU, you can do it. Uh. And so basically, we were
gonna follow that. We would just say that Wasi just expects Wasi and then just expects default CPU
tensors and then we would add like a DOT CPU function that returns another tensor. The thing that
again it would still run into is. Does like can we have like a constructor that doesn't require
passing in tensor data so that it when you call that GPU like we can still return something that is
a tensor. It's just like underneath the covers, like it knows it's smart enough to know, like, hey,
this is on the GPU. My like list U8's gonna be empty until the future and whatever, whether that
looks like the WASI IO thing or not, I think is sort of like an implementation detail of how we want
to do that. Uh, but I feel like that would be like the if you wanted to imitate what the most
framework to do, that would be the most logical thing to do, because as far as I understand, we
aren't start ever starting with tensors on the host side that are then getting past to the guest
side unless they're expecting to receive tensors from other specs and other sources, which the
moment I don't think so. But even then, I guess we could you could do something like every tensor is
by default created on the GPU. Umm. And that's still just me. You still end up with a copy of the
copy doesn't go away. It's just automatic if that makes any sense. But.

Mingqiu Sun: So like when you call create tensor, right?

Matthew: It's that like, even on like, forget about Wasi and then for example like let's say you're
in Pytorch, like you've assembled a bunch of like like a vector and RN in like RAM and then you call
create tensor and you pass it in like the like the ND array or whatever or you or you like build it
in place. I guess you in theory you can build it in place, but you're still doing that. Just makes
the copies 1 by 1. There's no magical way that you don't have to do like. OK, I'm gonna pin some ram
in the kernel and then I'm gonna do CUDA copy to move it on to the GPU. Umm yeah.

Mingqiu Sun: Yeah. It's always at some point gonna touch ram and less the tensors that output tensor
from a calculation, and even then you still have to load those tensors at some point onto the GPU.

Matthew: So is this the what the Andrew has?

Mingqiu Sun: Here are the constructor like you you have. You might be like this resource is GPU
resource and you read the the tensor data which is, you know, essentially the WebAssembly linear
memory, right? Or CPU tensor. It's it's not quite which is what I think Stuart is pointing out.

Matthew: And so I can give you the a specific example where this would break down in the current
model that we have specified here. So if you create like let's say you load a bunch of tensors and
then you pass them to the GPU. Your output tensor is gonna come from the GPU, so the only way you
could create this resource right now is you would have to take that output tensor from the GPU, copy
it back to main memory, create this tensor resource and return it. Which means that you will always
have to move A tensor. That's been that's the result of the computation back into main memory. While
that is not true for all the frameworks and so that means you cannot like get back A tensor, that's
a result of the computation. Without doing that copy, which I do think would be something that we
should address right? Like if I'm looking at the specific case where this breaks down right now is
like the output case where you want to keep that tensor on the GPU because you might use it as the
input to another model in the pipeline. Stuart.

Andrew Brown: Right, so, so, so if you remember the, the first thing that I demoed to you guys of
the of the, the stuff that I built was a stable diffusion unit that you run multiple times on a GPU.

Stuart Schaefer: And the last thing that you want to do is at the end of each iteration of the unet,
copy the tensor off the GPU. It's expensive and it adds an incredible amount of latency to the
pipeline, so by leaving the tensor on the GPU until you are finished with the unet it saves an
incredible amount of you know what's called work. So, as Matthew was saying like that, when you
think through the sets of cases that are here, you need control over that memory. If you make a set
of assumptions about where this thing is, and that copy is always OK, you get into trouble very,
very quickly and so like what I've been trying to say is that when you look at this tensor resource,
you kinda have to be explicit about which tensor resource it is. If not, we're going to get auto
copies by the platforms underneath it, because that's the only thing that they can do to be
completely. Let's call it safe. And so we typically try to do and why, you know, Pytorch has the dot
CU dot GPU and it allows you to be explicit because you want to control only the developer knows
their intention of what's next. And so I had that. That's the thing that led like, that's why I'm
worried about just saying something has to be a list U8 or an X, because there's two sets of cases.
Can I describe how this tensor would work this this resource thing?

Andrew Brown: Idea would work and the component model and I think it it that use case of like let's
say let's take the stable diffusion example. We're gonna run 20 iterations on the same, you know, on
a, on a chain of tensors, and we do not wanna do copies, right? Is that the? That's the. That's the
use case, right, Stuart? That you're mute.

Matthew: You did start.

Andrew Brown: Sorry, I'm muted.

Stuart Schaefer: I'm sorry, I apologize that we have this resource. OK, the resource points to set
of tensor data at tensor data is right now specified as a list that has to be in the memory machine
of the wasm. Yeah.

Matthew: You know instance, which means that the CPU tensor only correct.

Stuart Schaefer: Hey, Andrew, do you?

Matthew: So I think that's that's where the there that's incorrect I think.

Andrew Brown: OK.

Stuart Schaefer: Can I describe how this could do what you just described where the data is on the
the tensor, but Matthew you wanna say something?

Andrew Brown: I was just gonna ask, are the functions on the tensor?

Matthew: Do they allow mutation of the resource? Are they constant functions? This is so OK, let me
describe it now.

Andrew Brown: It will. It will answer that question the the umm how this is set up is that for us to
construct a tensor on the guest side, I'm gonna use the term guest to mean inside WebAssembly. Use
host to be like over in backend land in so on the guest side in web assembly the only way to build
the tensor is we have to give it a list of U8 right? And now I understand that we might want other
ways of maybe when passing a file or a stream or some other thing, right? Here and that, but let's
put that to the side for a moment, OK? However, while you can create uh tensor on the guest side,
you can also create a tensor on the host side and you don't necessarily need to use this
constructor, right? That tensor could be created however you want. And uh, in fact, if we look down
here in compute or whatever, get output whatever we called it right when we retrieve the output from
a computation and inference, we could get a completely. Uh, we'll get a a component model resource,
right? But what that resource is and how it works on the host side is not. You know there's nothing
in web assembly that that constrains that right. The only constraint is that that resource must be
able to answer these three questions. What are your dimensions? What's your type? What's your data
and so? I think you have to have the point question.

Matthew: And the problem is that with the but the problem is that if it answers that and it says I
have a tensor data of a list of U8, it implies that something has to be able to manipulate it on the
guest side as if it's a list. Uh.

Stuart Schaefer: Right.

Andrew Brown: And So what this function does right now is it. It would just simply copy the data
over. This isn't. This isn't the mutation that Matthew is talking about, right? So this would just
give you a dump of what the data is and if you wanted to mutate the data we need a new function.
We'd need a mute or something like that, right? No, I I think Stuart is saying is like when you call
data.

Matthew: Can I?

Andrew Brown: Can I finish, though? Hold on. I I I think what you're most I think what you're saying
is fine like like I think yeah. OK.

Matthew: Let me let me finish the thought.

Andrew Brown: Right. And then let's talk about it. So if if I want to inspect the data I need to
copy it over into wasm linear memory, if I wanna mutate the data which isn't present here like you
know it's not visible. I would also need to copy it into wasm linear memory, right? Because that's
the only way we're gonna actually access things from the guest site, right? Currently there's no
zero copy view into host side memory and so in both cases we like are constrained by WebAssembly
into doing a copy, and so my argument here is well, as long as the user doesn't call data or that
other hypothetical function to mutate the data, then they haven't done a copy right. If they do want
to it actually inspect the data, then they must right now do some type of copy and so in the case
where Stewart's saying, well, we don't, we don't wanna copy that date over. Well, then we shouldn't
be inspecting the the bytes of that tensor, right? That's kind of OK. Does that thought make sense?
I was going to say that.

Matthew: Yes, but but lacks one.

Stuart Schaefer: Yeah, that's what I say.

Matthew: Matthew.

Andrew Brown: It lacks one property which is again because that.

Stuart Schaefer: Sorry, all that money go first. Matthew and then Stuart.

Andrew Brown: Does I think I was gonna say the exact same thing.

Matthew: Stuart is, is that like I think on here you you should add like a I don't know location or
residency or whatever that's like CPU, GPU, NPU that tells you where the tensor lives. Uh-huh.

Mingqiu Sun: So that you can right?

Matthew: Right, yeah.

Andrew Brown: Because otherwise, like you, don't know how expensive that call is gonna be, cause
like it can either be like basically free or it can be alright.

Matthew: Yeah.

Andrew Brown: Right.

Mingqiu Sun: Yeah. OK.

Andrew Brown: Well, now I'm copying like a MB off the GPU and the like, and you don't know which one
right now.

Matthew: Yeah, that's right.

Andrew Brown: I'm with you on that. We could have additional questions to ask like location. Sure,
you want to say something? Yeah, I guess it just what gets hard here is what I'm I guess confused by
and maybe I'm missing something is the.

Stuart Schaefer: If you say that this thing is a list you ate OK, and if you touch it, it gets a
copy. The problem is, what is the semantics of this thing? If what it is is a CUDA memory stream,
OK, you're gonna require any implementer of this spec to basically have to write a bunch of code
that does anything to a list of U8 to that CUDA stream that's legal, right? When in reality, what
you have are some very specific primitives for manipulating acuda memory stream, right? Yep, Yep.

Andrew Brown: And so, by acting and pretending that it's a list, you eight, I think you get yourself
into a little bit of prompt trouble.

Stuart Schaefer: It's fine for CPU tensor because that's what it is, right? Mm-hmm.

Andrew Brown: But I'm just trying as long as you pretend that this thing is a pretend that this
thing is opaque when it's not, you know you're probably gonna get a little bit of trouble.

Stuart Schaefer: That's OK.

Andrew Brown: So Matthew actually had an idea a while back, which was why don't we not do list of U8
right? Why don't we have accessors? You could imagine this data function as you know, get item where
you pass in an index. Or maybe you pass in a multi multidimensional index, right? And you. But, but
I think, Andrew, what you're what you're going to quickly get to is Dave's proposal, which is you're
gonna get to something which is an IO stream.

Stuart Schaefer: Sure.

Andrew Brown: Right.

Stuart Schaefer: Right, so I'm saying.

Andrew Brown: And so.

Stuart Schaefer: So I think I I think what I'm trying to say is this is that I will admit my naivete
on the sort of you know what is the implications of resource and variant and option and all the kind
of things in WebAssembly. I think the simple question is this. Are we better off using a native
stream type from WebAssembly like Lazio? Or can we define something which is? Like I said, the way I
read the spec is is I can define a variant and that variant says I'm giving you back a handle and in
some cases this thing is a list and in some cases this thing is a opaque reference. Anything other
than that, I think you're going to get into some like the deep water because like they it'll, you'll
figure it out. You'll write. We'll write down technically in in English language what the
specification means, but because the code can do anything, getting some deep trouble. So let's go
down that route.

Andrew Brown: Tensor data. We make it an enum and it has two variants for right now. Uh one is. It's
a list of U8, so you could get back when you ask for data. You could get back a list of you 8, or
you could get back opaque, another opaque resource which is like also tensor data or whatever with
that resource. Don't you want to at some point inspect it or change it? So.

Stuart Schaefer: Or or how do you interact with that that opaque resource?

Andrew Brown: So that's why I'm asking you. So, but that's what trying to say is it is, if you think
about the definition you've had earlier of host and guest side, OK, are you going to in the guest
side being manipulating that tensor if it's an opaque resource?

Stuart Schaefer: Do you wanna do?

Andrew Brown: Do you need to so if? And and I think my head like this which is that like yes, there
are other people that are like working on stuff to manipulate it and whatever.

Stuart Schaefer: But like, let's let them define that not us and say like, hey, if you can give us a
resource and it's like this and you create some other functions, I don't think we should because
most of what those things are defined outside of the neural network domain, they're in the general
graphics or general high performance compute domain. I don't know if I fully understand what you
mean there, because I would have assumed that at at at some point in the chain of inferences, maybe
you get to the end and you say I.

Andrew Brown: Now I have my output tensor, but it's still in that it's still on the GPU. Rib.

Stuart Schaefer: Right.

Andrew Brown: I'm saying that the model I have in my head is that the the host side is gonna be,
let's call it some browser with a JavaScript engine that's driving some form of, you know, creation
of these tensors and all the rest of this other kind of stuff and setting up the pipeline and then
driving calls into Wasi.

Stuart Schaefer: And maybe there's going to be some additional polyfill and other code that people
do inside of wazzie to make it way easier for JavaScript to call into. Right. Either way, where is
that tensor created? If we're going. Yeah, I'm not sure I'm I'm.

Andrew Brown: I'm not sure I follow you. I've tried. I tried hard but. OK.

Stuart Schaefer: How can we just do something like a pie torch?

Mingqiu Sun: You know, create a like a abstract tensor and it's assuming like a default to CPU and
then to say 2 device right? I added two device function so that's like a copy function to you know
two GPU, two CUDA or to whatever, you know, TPU. Yeah, I I think that that was what I was
suggesting.

Matthew: I think the IT sounds like they're is some complications. If you want to interact with a
GPU tensor to do stuff to it without copying it out into linear memory, uh, my question there would
be is, is that common right? Like most like, I can definitely see GPU tensors getting passed into
next stages of the pipeline and that problem would be addressed because you would just get back a
GPU tensor. But what I don't know is I have not encountered code that like actually mucks around
with like the CUDA stream to update A tensor in place. But if you guys have seen that, then that
would be a good data point to know. I would say as a starter we could basically be like, OK, let's
add in this type cause we know for sure that will immediately buy us a ton of stuff and then it
sounds like we need to understand the the Kurdish stream stuff better before we make a a larger
proposal and cause like one of the things I was saying is like if we need to like change it, maybe
we have like a separate subsection of the spec where we like, right. Because like right now, at most
I suspect it will be like CUDA. AMD and Intel will be like specifying like these type of
transformations and so like. What's the shared set of transformation? It almost turns into like if
you want to have like have these modifications like have some generic abstraction over them that is
large enough to be its own spec that also, but it shares the resource type tensor and it has a bunch
of functions where you can give it a tensor and then the transformation you want. But then again,
that's just like off the top of my head. And I'm saying we should think about that. Because it
sounds like there is a gap in understanding on that. But I think that the one agreement we have is
like add in like this tensor type so we know what we're dealing with and we can avoid the copies. So
if you get back a GPU tensor, you can be like, oh, this is already a GPU tensor. I shouldn't call
and get the tensor data if I don't really need it for anything, but I can go and use this in the
next stage for my pipeline, which should speed up all the pipelines that stable diffusion. The
perfect example because you have to run it like 30 times before you get the output. Umm. So. So
yeah, that's that's my thoughts is like we should at least make that that change because that one I
think is well understood and has immediate benefits. And then it sounds like there's more discussion
around, like how much, how much we expose the OR how we expose or if we want to expose the lower
level operations on the akuda or whatever Intel AMD equivalent of CUDA end up being. Can I?

Andrew Brown: So I let me make sure I understand what you're saying you're saying. Let's make an
enum here. Have two variants. Once a list of you 81's opaque additional resource, right? Umm, it
sounded like we didn't even need to do that.

Matthew: Right.

Andrew Brown: It sounded like we won't want an enum that's like CPU, GPU, NPU that that is added to
the constructor.

Matthew: So like where it says right now this is like tensor type, it's it's like telling you like
the data type. Yeah.

Andrew Brown: It's like you need like a tensor residence.

Matthew: Yeah, yeah.

Andrew Brown: Or something, and then that one has like MP GPU.

Matthew: There's a bunch of different ways you can handle the piece you talked about. You could
either be like OK, if it's a GPU tensor and you call tensor data, you will get empty array back or
something and then if it's, if you want to access it you have to do like CPU or something to get a
CPU tensor and then you can then access it the so it's it explicit if you if you don't like that
then you could be like well you have to check whether it's GPU 1st and then if you call tensor data
or you can make the function return a result. There's, like a bazillion different ways of doing it.
I think that the high level thing is the GPU tensor. You have to do something explicit to get it to
do like the copy out and the and that there's an easy way to do it. You could have a result that
returns an error if it's a GPU tensor saying hey, Are you sure you you wanna copy this or hey, you
need to like copy this to the CPU manually or or return an empty array and whatever makes most sense
results probably more friendly to developers. They would see an error that's documented. OK, so I
think what I heard there is like there's a bunch of different ways to do this.

Andrew Brown: And uh, I heard the yeah, we could have a like a location or residence function. Uh,
David? So I mean, let's you ate tensor data.

David Justice: It might be it might be useful if if we do end up using like a a stream or something
like that, then you know we could instead of having a variant there of like list you 8 or stream, we
could just have a constructor for a stream of listview a. So, like, hey, if you want a your tester
data we can construct it. Just give us a list of eight and then we return back the resource already
constructed. With this, it's just an in memory representation of it, right? Otherwise you can create
your own implementations of those and and perhaps that ends up being a GPU implementation where you
know you're not gonna read the entire thing necessarily. You could read the entire thing, but it
it's not necessary. You know, not necessarily at all at once and the thing. OK.

Andrew Brown: Can I now? OK, I I have this like pending question. Uh. So if regardless of the method
by which we don't copy the data, uh, if we have a tensor that's stored on the GPU, isn't it true
that at some point we want to be able to access it within WebAssembly, right? We wanna access the
contents of that GPU tensor within WebAssembly memory. Is that the case? Right.

Matthew: They can. You repeat that. I just missed the GPU. I.

Stuart Schaefer: OK, so let's, let's say we have a tensor that is stored on the GPU by some via some
mechanism, right?

Andrew Brown: Yep.

Matthew: Uh, we get to the end of the work that we wanted to do and now we have a handle to that GPU
tensor.

Andrew Brown: Isn't it true that we want to be able to inspect the contents of that tensor inside
the WebAssembly guest? Uh.

Matthew: I would argue to Matt.

Stuart Schaefer: To Matthew's point earlier, there is a place where we say we explicitly want to
copy it out of the GPU and treat it like WebAssembly memory, but we don't want to use web assembly
code to be mucking around with GPU memory. Yeah.

Matthew: So at some point you will want to copy it, but like you, that's not necessarily after one
execution. So you like again, you have. You want to defer to the developers knowledge of whatever
whatever sort of root Goldberg, uh AI system they they built through train, they'll know when to
copy it out. Right.

Andrew Brown: I said in the kindest way.

Matthew: Yeah.

Andrew Brown: Yeah. So I some point there will be some people who want to copy the data and I think
to Stuart's point, uh, we want to be very, very clear that, hey, this is gonna be expensive, man.
We're gonna move all this GPU stuff over into web assembly in your memory. Watch out. Hey, you know,
like that's the sensually the thing here we wanna make. So when you're talking about CPU DOT GPU
accessors and Pytorch like, it's very clear and Pytorch that I'm there it is. So I'm moving the data
and so my initial premise was well, I thought this was enough to be like, ohh I'm gonna go grab the
data. Maybe this is gonna be expensive, but am I hearing we need even more? We need an additional
step to just make it really clear to them, you know, hey. Watch out, you know. Warning right? Like
having a having the we kind of discussed the ENUM variants here right? Like if you you had a an
additional like I grabbed the data, it's an opaque resource. Then I do the additional copy and it
could even be copy or whatever. Is that what's needed here? I guess for for me, I guess I'll ask the
question which is again like tell me what I'm missing here, but what you're saying is that I could
create a tensor using a constructor that is not this one right here and just tell someone that my
memory region is a tensor and you're OK with that.

Stuart Schaefer: Right, that could that could be returned by the the back ends, right?

Andrew Brown: They could. They could return tensors that have never been materialized right until
someone calls data. Well, sure, but someone has to.

Stuart Schaefer: Someone has to do that, which means that not the back end itself, but someone in
general like, let's call it in the, you know, in the hosting platform of written, as Matthew called
it some Rube Goldberg code that says, OK, I'm gonna create these three tensors. They're GPU tensors,
by the way. Right? And then pass them to this inference function. Well, as a part of the the models
that you're using, right?

Andrew Brown: The input tensor isn't the same output tensor, right? No, no, no.

Stuart Schaefer: Right, the output tensor could be a new handle.

Andrew Brown: But again, what I'm saying is this is.

Stuart Schaefer: Let's imagine that. Sure.

Andrew Brown: What you have is is some stable diffusion simple case, OK?

Stuart Schaefer: And what I'm telling you is that the memory for this thing was created not
necessarily in the WebAssembly itself. Let's say it was created by some other interaction with the
platform and it's already on the GPU. Yeah, that's completely possible.

Andrew Brown: Even today is what I'm sort of starting. But I'm trying to figure out.

Stuart Schaefer: All you're saying is I'm gonna give you this thing on a call to tensor and you
don't care. And as long as we can do that, I'm totally fine. And if what you're saying is at the
moment I touch the data, the call the data function will copy it over. OK. Then I think the reality
is all we need is Matthews suggestion, which is we should mark the tensor as this is not CPU data.
So the person who's touching it knows what they're gonna do. Yeah.

Andrew Brown: But my understanding was that like by by saying this thing was a list UIT it had to be
a list U8.

Stuart Schaefer: I think only on the web assembly side that that's the only way to construct these
is to relist you 8 and so that's what you're seeing here, right?

Andrew Brown: Right, yeah.

Stuart Schaefer: Uh, this constructor is not used when that tensor is created elsewhere, right and
right.

Andrew Brown: Elsewhere. Perfect.

Stuart Schaefer: And so I was kind of trying to, yeah, like like you said, like, the only copy is
this.

Andrew Brown: This data is the only copy. Maybe we just need to be really, really clear here and the
docs and be like, hey, when you access the data, uh, you don't know where this has been streamed
from, right? Like. If this is debug data, yeah, it's it's gonna hurt.

Stuart Schaefer: Yeah.

Andrew Brown: Yeah. Yep.

Stuart Schaefer: And then and then we maybe add a what do you want to call it Matthew like?

Andrew Brown: Reset. I just think your location, but you you residency is maybe a better term. Yeah,
I was, yeah.

Matthew: Yeah, residency seems fine. I'm trying to the most checking out right. Yeah. David, does
that you kind of you kind of wanted more maybe you wanted additionally?

Andrew Brown: Ohh the the only thing that and the only thing about that that you know kind of
concerns me if if you're in a resource constrained environment.

David Justice: Reading the whole thing back is is that going to be? Is that gonna be a problem? Umm.

Andrew Brown: Is there a case where you actually do need to stream back chunks as opposed to reading
the entire thing and putting that all in memory?

David Justice: And then you know, doing something with it. Yeah.

Andrew Brown: So as you were talking earlier, David, I was like, well, yeah, maybe so this the WASI
IO thing. I one day we'll have streams of U8, right one day. Right now we have lists of Fuentes, but
one day we'll have streams of UTS and you'll be able to like create a stream of U8 from a file or
from a network resource, or for anything right? And if we were passing in streams of U eights and if
we were passing out streams of view eights, right, maybe then the resource constrained thing is like
on both sides we we get the we get the benefit you know. Yeah.

David Justice: And that makes sense. And maybe that change comes with a, you know, Preview 3. Yeah,
yeah.

Andrew Brown: So pretty three, we're going to be landing native base sync and you know streams come
come along for the ride.

David Justice: So maybe that's when you know there's gonna be massive breaking changes across API's,
right? Mm-hmm.

Andrew Brown: So maybe that's when that lands.

David Justice: OK, alright.

Andrew Brown: Umm. Let's go through the notes the the chat here. Matthew, you wanna explain that one
or make sure you had a comment before that? Yeah, I think, uh, it's a similar to what the Matthew
has.

Mingqiu Sun: Uh, you know, by default you can create a tensor on the CPU side with, you know, buy
that input right? And then you can do the conversion to a device tensor resource by calling a
function you know the two device or dot device or something like that. And then you can convert back
and forth, you know with the resource does something to the target device. So that's kind of the
main idea. Right, right. So add explicit ways to say I wanna move this to a different place, yeah.

Andrew Brown: And the default is like you create from.

Mingqiu Sun: You know a buy a list of USA to CPU tensor first you know and then you can move data
around with those functions. Umm OK.

Andrew Brown: So I think we had a good discussion on this. I I understand a lot better. Stuart,
we're, you know. Yeah, the concerns and stuff like that. Same for Matthew 1 remaining bit like what
do we do about it? The tensor type currently returned by the the type function. Matthew, didn't you
have some concerns about whether this should even exist or like maybe you want it to be a string or
what? Yeah, I was gonna say.

Matthew: Having it be an enum. Uh is unnecessarily constraining, given that we are not going to use
these values at all in the actual in actual the actual standard. In other words, like right? Cuz if
someone creates an array and packs it full of FP32 and then tells us an FP16, we will just happily
pass that on, and we're not, we're not doing any like enforcement as part of the standard, so if
we're not doing any enforcement, we could just make it a string so that if someone packs like. FPGA
or FP4? Something insane like that and they figure out how to do it in wasm memory that they could
happily pass that on to the underlying framework. And like if they invent something new like BF 16
or whatever, they can happily go and do that and they can just pass the correct string that they
know that the underlying framework will support, and that doesn't matter to us. David.

Andrew Brown: Is there any kind of transmutation that we're gonna need to do?

David Justice: So do we need to know say data type, size and brightness? That all gets delegated to
the developer, but that's basically, I mean that was my understanding of like we had a previous
discussion with students thought about this and like at the end of the day really like we're not
providing any code that lets them encode right cause like half these types don't even exist in
wasn't.

Matthew: Yeah.

Andrew Brown: So like we're not providing any code to like make these types.

Matthew: So like all this is like decoration and then it's up to the developer to figure out how
they make an FP16 or BF16 right like and give if that's. And they better do it right, or it's gonna
blow up, right.

Andrew Brown: Sort of. Yeah, I yeah.

Matthew: And and I feel like with Stuart's changes on errors, if they get back an error, that's like
legible from the framework. That's like, hey you, you said that it was a FP16 and then you passed me
and you said that the mention was like 10, but rather than receiving like 20 bytes, you sent me 40.
Here's your error, but all those errors are going to be bubbling up from the underlying framework,
and so yeah, so my my point was like is enough just means that people can't use all the types that
they might wanna use given that we are not going to be putting any uh limitations on that. Right.
And also not all the frameworks named their types identically. Yeah. Yep. And and the real problem
is that the actual number of tensor types that are there is growing with with incredible bounds due
due to quantization and other crazy things going on right the I mean, I can tell you at least thirty
others that are not in this list.

Stuart Schaefer: Yep.

Andrew Brown: Is there something we can do to help people be successful in using these?

David Justice: Because that's one thing that actually I find helpful and maybe I'm not a good use
case for this, but it's kind of a happy path. It's like you see. OK. Hey, here's one that should
translate well, right? OK. Yeah. So.

Stuart Schaefer: I just type in the email because it fills out the amine VS code, right?

David Justice: But so did David.

Stuart Schaefer: That's where I started at the beginning of this call was to say exactly that is
that there's really 2 cases we're talking here is one is gives people that great happy path. We can
line up tooling and UI and all this great stuff, and then there's another one which is just opaque.
Pass me a string or whatever you want, like it's just going to be completely opaque, right? And so I
kind of wanted to have this thing that said, I have two kinds of tensors. One is strongly defined.
You can mess around with it in web assembly. You can do all this stuff, and here's what it is. And
then here's this other one. It's just a resource and be careful what you do with it, right? And if
you touch these following functions, it'll come out of what your native weird thing is and copy it
into WebAssembly become acts. I love that right, but I'm saying is that there's really two use
cases. There's that great, happy path. And then there's the Rube Goldberg. This Matthew called right
and he got a support both. So you don't really have a successful thing. Yeah.

Andrew Brown: So.

David Justice: And that that explains why you went down the resource for tensor cause cause this was
part of it, right, Stuart, is that?

Andrew Brown: Yeah. Yeah. Yeah, that's why I said like I I I'm.

Stuart Schaefer: I'm. I'll call it. I admit my naivete. With the with the having the thought through
what's the right way to actually model it right? Yeah.

Andrew Brown: But when I think about the requirement, the requirement is to have these two
variations, one which is strongly typed, simple native Wasm platform and another which is kind of
relatively opaque.

Stuart Schaefer: But the developer is going to write some code where they better know what they're
doing or bad stuff is going to happen. And they generally they know that you touch a CUDA stream.
You better know what you're doing. David, you wanna say something?

Andrew Brown: So this tensor type then just become a variant.

David Justice: It's either enum or string. See, I don't know in the component model like like.

Andrew Brown: Here's one thought. Right, let's let's just keep FP32FP64I32 and I64 because those are
well defined types and WebAssembly, right? We could keep those if we had a variant that was like,
hey, give me a string that where I can put in, you know, you know that people four FP, 2, whatever.
Right, like I can do whatever I want, but I don't know if in the this component model WIT language
we can actually do that. Uh, but. But would that essentially solve the problem if we have? But.

Stuart Schaefer: Here's the four happy paths types, and we have the.

Andrew Brown: But I I think in Andrew, what I'm suggesting is I think that the thing that we should
do right is take some pencils, right, and write up with, call it three or four or five different
variants in wit and see what these things imply.

Stuart Schaefer: Get yeah.

Andrew Brown: And if we can convince ourselves that they mean, you know what we're hoping for,
right.

Stuart Schaefer: And maybe like I'll write some Dave, you can write some. Matthew can write some. We
can compare notes and say OK, this is what mine was meant to say and convey and what's the right
representation? Yeah, let's do that.

Andrew Brown: I don't know the answer to that, haven't really sort of thought down that depth and
the exercise, but I'm hoping that the big questions with two minutes left in our meeting is I think
we all understand the goal, right, like we all agree with the core that core goal, right?

Stuart Schaefer: Yeah, let's, let's talk.

Andrew Brown: Let's bring some concrete wit to the next meeting and share it right. And so, uh, do
you feel good about that, David, or you're like you're good. OK, so uh, the party? The Whig Party?
That that'll make it easier, I think, to to say like, is this possible? Will this work? So you guys
get Matthew you good with that? Yeah.

Matthew: OK, let's bring some WIT.

Andrew Brown: I'll put that on the agenda for next time. Uh, any other comments before we go,
Justin? Plause, Johnny. Yeah, yeah, absolutely. Code. Yeah, let me thumbs up the code. Wins help. I
can't. Alright guys, well I will put up a summary of this meeting and set up an agenda for the next
one. Thank you for taking the time to collaborate on this and I will see you next time. Alright, see
it.

Matthew: Thank you, Andrew.

Andrew Brown: Thank you.

