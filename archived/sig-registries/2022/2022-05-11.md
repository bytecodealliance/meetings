## May 11th — SIG Registries meeting

|          |      |
| -------- | -------- |
| Attending  | Radu Matei (Fermyon), Lann Martin (Fermyon), Brian Hardock (Fermyon), Peter Huene (Fastly), Luke Wagner (Fastly), George Kulakowski (Fastly), Roman Volosatoys (Profian), Nathaniel McCallum (Profian), Guy Bedford (?), Johnnie Birch (Intel)
| Note Taker | Radu Matei (Fermyon) |


### Agenda

- welcoming new SIG members and attendees
- next week's meeting — KubeCon week - should we cancel?
- the publishing API proposal
    - discussion — https://github.com/bytecodealliance/SIG-Registries/discussions/1
    - PR — https://github.com/bytecodealliance/SIG-Registries/pull/10
    - publishing and static assets — https://github.com/bytecodealliance/SIG-Registries/discussions/9
- namespaces - https://github.com/bytecodealliance/SIG-Registries/discussions/8
- sigstore - https://github.com/bytecodealliance/SIG-Registries/discussions/7#discussioncomment-2703351
- meeting at the Cloud Native Wasm Day next week

### Notes
- next week KubeCon week
    - decided to cancel the May 18 instance of this meeting next week
- the publishing API
    - Lann: quick overview of the publishing API. The basic idea is that it follows a RESTful model: you first create an unpublished release. It requires you to upload the content (either directly to the registry, or the URLs where the content can be found). The registry validates it matches the content digest. After, you sign the release, create an actual release, and the registry validates it.
    - Lann: there have been some discussions around content digest — not sure the discussion needs to be in great detail for phase 1.
    - Lann: potential to integrate with some services coming out of the sigstore project.
    - Luke:
    - Lann: if you have to send both, you need to send two different objects in the same POST request. There might be some other argument with something like signed upload URLs by registries, which is very specific to things like S3, but not intrinsic to the design.
    - Nathaniel: why not use a format like JWS then?
    - Lann: not sure I understand the question. For my understanding of the phasing, we shouldn't focus too specificly here. I'm not trying to propose a specific format.
    - Nathaniel: I'm suggesting that any signature over that hash is going to contain the hash, so you can upload that instead of the content digest. That content digest is always going to be a field in the signature. If you are not going to sign the release, you commit to the digest ahead of time.
    - Lann: the signature is not of the content, but of the entire "publish" message. The details of how you sign JSON are controversial. The proposal is to sign the release manifest.
    - Guy: would a package consumer verify it?
    - Nathaniel: when you POST to /releases, you either upload a naked manifest, or a signature which itself contains a release manifest.
    - Lann: that is a possibility.
    - Nathaniel: the content type in the request can distinguish between the two types.
    - Lann: if you use somthing like JWS, that is totally possible.
    - Nathaniel: I would suggest we use JWS, because we are using JSON, which is also a practical decision. But I am not too strongly opinionated here.
    - Guy: is it worth considering more transactions by the publisher to be signed this way? i.e all publish requests are signed.
    - Lann: that would be an API authentication layer. This is an end-to-end signature, which would be verified by the consumer. SIgning the whole request would be a server auth operation.
    - Nathaniel: package type components - is this the only component type?
    - Nathaniel: it might be nice to make `contentType` -> `mediaType`
    - Roman: you upload content URLs. Over time, what happens if the URLs become invalid?
    - Lann: the URLs are not part of the signed manifest. They can be mutated to point at another mirror. The content digest is what authenticates a URL.
    - Roman: so the digest of the signature only contains a subset of the which fields?
    - Lann: adding the content URLs is a later phase.
    - Nathaniel: if we standardize on JWS, we could condense some of the fields
    - Lann: totally open to using JWS for this.
    - Nathaniel: is anyone objecting to standardizing on JWS?
    - Radu: is this a phase 1 discussion?
    - Radu: do we agree on the general shape?
    - Nathaniel: I would like to do a detailed pass, but I don't see any major issues immediately.
    - George: we can have an agenda item two weeks from now to agree on this.
    - Nathaniel: the lack of crypto agility seems problematic.
    - Lann: is this the kind of question we feel is a phase 1 or 2 type of question?
    - George: it does feel right on the edge.
    - Lann: I'm fine changing this document, I want to understand the process.
    - Nathaniel: the release manifest itself could be replaced entirely by JWS with a protected header. This would be a major structural thing, but addresses crypto agility for hashing.
    - George: would this change modify the number of HTTP methods to achieve the same result?
    - Nathaniel: no on the methods, but likely on the data. If we want to advance just on the shape of the HTTP methods, I would call this a phase 2 method. Otherwise, it's a phase 1 question.
    - George: we discussed crypto agility in this group before. I am not sure we all agree on what is necessary and what is needed to achieve that. Do we want to spend time on that separately?
    - Nathaniel: what might this impact? It might be good to discuss how we want to treat failure. If we were to lock down on SHA256 and it becomes weak, what is the recovery story? How does agility affect this, and can we prevent these problems?
    - Lann: my assumption is that a failure of SHA256, for example, would require revving the entire log data structure. Whether we can design a system that would be agile, we would still have to regenerate the existing data.
    - Nathaniel: you would just have to
    - Guy: you could have co-existence of old and new manifests until you
    - Lann: you would still have a client compatibility issue.
    - Nathaniel: the point of crypto agility is to prevent this. You take all the data, hash it using mulitple algos, and include multiple content digests. MD5 and SHA1, for example. WHen MD5 is weak, you use SHA1 to validate. Only when both become weak, you would rev the log.
    - Lann: so you would be signing both digests?
    - Nathaniel: you can make "content digest" an object instead, and have multiple algorigthms and multiple signatures.
    - Lann: so it would be an optimization.
    - Nathaniel: if the content is foreign, you have a real problem. If you have content in a remote store and it is hashed using an outaded hashing, you have no idea whether the content you need to update the log is correct.
    - Lann: my only concern is that I don't know if we can pick the right algos.
    - Nathaniel: the point is having a flexible data type so uploaders can choose what algorithm to use. Different customers will have different requirements.
    - Lann: the action to take is replace the content digest string with an object, which inclides the algorithm.
    - Nathaniel: following the JOSE so we don't reinvent the strings for this. This gates us on whether we want to adopt this ecosystem. We could adopt the names of the hashes independently.
    - Lann: digest agility is a good change to make, ^^ seems like a phase 2 question.
    - George: I would appreciate a more worked example of what it would look like for a user, maintainer of a registry.
    - Nathaniel: the registry does not validate any of the hashes?
    - Lann: this depends on whether the registry needs to fetch some portion or all of the content to extract metadata. Potentially, it does not have to digest all of the content. There are scenarios where the registry wouldn't have. But if we are to index on say imports, the regitry will have to fetch.
    - Nathaniel: if we allow multiple hash types, and if the server does some validation, it is a 3-way negotiation, and all parties have to support all algorithms.
    - Lann: it doesn't seem like the registry absolutely needs to, but I am not sure.
    - George: error handling in general — how much do we need to go into this, are there any decisions we need to make here?
    - Lann: it is briefly addressed in the doc, but there is nothing obvious that wouldn't fall out naturally
    - Guy: if the publish release operation is async, if there is an async error is there a message to the original publisher?
    - Lann: I have no immediate opinion right now. Ideally, it wouldn't be an async operation, but I'm not sure.
    - Guy: "processed at" or similar metadata could be useful.
    - Lann: there is room within the unpublished release resource to add this sort of thing.'
    - Luke: is the idea that when you implement this API, you could upload parts to S3 and give the URL? And is it optional that I could upload parts of the package to the registry itself?
    - Lann: there is a whole discussion around static assets that goes for this. I would hope that this publish API would support a storage strategy that lets you store assets separately.
    - Lann: the different URLs are all mirrors.
    - Luke: would this API need to change to support static assets?
    - Lann: what would change is that the thing digested into the content digest would change. It could be an index of multiple files.
    - Luke: so not just a blob, but a directory of blobs.
    - Nathaniel: this raises a question about access control, particularly for remote assets. "Can I access the manifest information about a component" and "Can I access the content that is referred to by this manifest". So you could access one but not the other? This could be either a poor experience, or bad at securing this.
    - Lann: we could have a parallel discussion for this around the policies for what BCA would do. I am leaving out some details about access control.
    - Guy: could we consider updating the URL to be an object that could have credentials and custom markup formats? When using a URL as a standard content type, you are trending towards IPFS and tarballs. There is the question of whether you syupport custom schemes. Just throwing it out as an option.
    - Lann: "content URLs" could change to "content sources".
    - Radu:
    - Nathaniel: you could upload components that no one can download, which could be an attack vector.
    - Lann: is this something that can be solved at this level?
    - Nathaniel: only if you group the storage layer.
    - Lann: the security hole would be a DOS rather than allowing malicious contact.
    - Ralph: the auth question is an excellent one.
    - Luke: in say OCI, is the separation normal?
    - Lann:
    - Nathaniel: if this was an IETF draft, we would have a security consideration section what should happen around access control.
    - Ralph: we should draw the security considerations and what our recommendations are.

- sigstore
    - Lann: they plan to have public instances for transparent logs and OIDC servers, which aligns with our goals. It sounds they will GA on that in the next few months.
    - Nathaniel: no objection for an optional integration, but cautious for a requirement.
    - Ralph: their OIDC integration is fantastic. At scale, transparency logs make no immediate sense.
    - Lann:
